{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractor.py Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import xarray as xr\n",
    "import os\n",
    "from pyproj import Transformer\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "start_date = datetime(1950, 1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDates(time1,time2):\n",
    "    time_start = datetime(int(time1[:4]), int(time1[4:6]), int(time1[6:]))\n",
    "    time_end = datetime(int(time2[:4]), int(time2[4:6]), int(time2[6:]))\n",
    "    assert time_start < time_end, 'ERROR: time1 cannot be after time2, please check.'\n",
    "    \n",
    "    year1 = int(time1[:4])\n",
    "    year2 = int(time2[:4])\n",
    "    time_str = f'{time1}-{time2}'\n",
    "    \n",
    "    if year1==year2: sameyear = True\n",
    "    else: sameyear = False\n",
    "    \n",
    "    return time_start, time_end, year1, year2, time_str, sameyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkParams(depth, bbox, time1, time2, vars_sel, group, formats):\n",
    "    # Define Global Variables\n",
    "    global time_start, time_end, time_str, year1, year2, sameyear, bbox_g, bbox_str, depth_g, vars_g, group_g, formats_g\n",
    "     \n",
    "    # Check dates\n",
    "    time_start, time_end, year1, year2, time_str, sameyear = checkDates(time1,time2)\n",
    "    print('Time Range:', time_str)\n",
    "    print('Same Year flag:', sameyear)\n",
    "\n",
    "    # Check BBOX\n",
    "    bbox_g = [int(bb) for bb in bbox.split(',')]\n",
    "    bbox_str = '.'.join([str(b) for b in bbox_g])\n",
    "    print('Bounding Box:', bbox_g) #, bbox_str)\n",
    "\n",
    "    # Check depth\n",
    "    depth_g = depth\n",
    "    print(f'Depth: {depth_g}m')\n",
    "\n",
    "    # Check Vars\n",
    "    vars_g = vars_sel.split(',')\n",
    "    print('Vars:', vars_g)\n",
    "\n",
    "    # Check 'group' flag\n",
    "    if len(vars_g) > 1:\n",
    "        # the flag 'group' is required in this case\n",
    "        group_g = group\n",
    "        assert group_g is not None, 'The flag \"--group\" is required when multiple variables are selected.'\n",
    "        print('Group files per Var:', group_g)\n",
    "\n",
    "    else: group = False\n",
    "    \n",
    "    # Check Formats\n",
    "    formats = [ff.lower() for ff in formats.split(',')]    \n",
    "    formats_g = []\n",
    "    if 'csv' in formats: formats_g.append('CSV')\n",
    "    if 'netcdf4' in formats: formats_g.append('NetCDF4')\n",
    "    print('Output files format(s):', formats_g)\n",
    "    assert len(formats_g) > 0, 'ERROR: Output file format entered is wrong. It must be \"csv\", \"netcdf4\", or \"csv,netcdf4\".'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDDS(url_info, year):\n",
    "    # Get dds info, and assign max dimensions to TIME and DEPTH\n",
    "    \n",
    "    pc_dim_dict = {}\n",
    "    time_stop_dict = {}\n",
    "    depth_stop_dict = {}\n",
    "\n",
    "    # get all content of the server_url, and then filter it with year and available platforms\n",
    "    page = requests.get(url_info[0])\n",
    "    webpage = html.fromstring(page.content)\n",
    "    \n",
    "    urls_filtered = [p for p in webpage.xpath('//a/@href') if p.endswith(f'{year}.nc{url_info[1]}.dds')]\n",
    "\n",
    "    for u in urls_filtered:\n",
    "\n",
    "        dds = f'{url_info[0]}/{u}'#; print(dds)\n",
    "\n",
    "        # Find platform code\n",
    "        if len(url_info[1]) == 0: pc = dds.split('_')[0][-2:] # nmdc case\n",
    "        else: pc = dds.split('_')[1][-2:] # t2_hyrax case\n",
    "\n",
    "        pc_dim_dict[pc] = retrieveDDSinfo(dds)\n",
    "\n",
    "        time_stop_dict[pc] = pc_dim_dict[pc]['TIME']\n",
    "        depth_stop_dict[pc] = pc_dim_dict[pc]['DEPTH']\n",
    "\n",
    "    assert depth_stop_dict.keys() == time_stop_dict.keys(), 'TIME and DEPTH Keys error. Please check.'\n",
    "    \n",
    "    return pc_dim_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPositionDict(pc_dim_dict, url_info, year):\n",
    "    # Extract data and create position_dict\n",
    "    \n",
    "    position_dict = {}\n",
    "    \n",
    "    for pc in pc_dim_dict.keys():\n",
    "\n",
    "        coords_str = getQueryString(pc_dim_dict[pc], keylist = ['TIME', 'LATITUDE', 'LONGITUDE']) \n",
    "\n",
    "        fix_lab = f'58{pc}_CTD_{year}' # label to use for this campaign\n",
    "\n",
    "        url = f'{url_info[0]}{fix_lab}.nc{url_info[1]}?{coords_str}'; print(f'Platform: {pc}. URL with Queries:', url)\n",
    "\n",
    "        remote_data, data_attr = fetch_data(url, year)\n",
    "\n",
    "        position_dict[pc] = {'data': remote_data, \n",
    "                             'data_attr': data_attr}\n",
    "#     print(position_dict)    \n",
    "    \n",
    "    return position_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePositionDF(position_dict):\n",
    "    # Load locations (LONG & LAT) and TIME of all measurements in a position_df_raw (includes duplicates)\n",
    "\n",
    "    position_df_raw = pd.DataFrame() \n",
    "\n",
    "    for key in position_dict.keys():\n",
    "        test = pd.DataFrame()\n",
    "    \n",
    "        test['Longitude_WGS84'] = position_dict[key]['data']['LONGITUDE'].data.astype(float)\n",
    "        test['Latitude_WGS84'] = position_dict[key]['data']['LATITUDE'].data.astype(float)\n",
    "        test['Time'] = position_dict[key]['data']['TIME'].data.astype(float)\n",
    "        test['Platform'] = key\n",
    "\n",
    "        # Convert TIME from float to datetime\n",
    "        test['Time'] = [start_date + timedelta(t) for t in test.loc[:,'Time']]\n",
    "        length = len(test[test['Platform']==key])\n",
    "        print(f'Platform {key}: {length} measurement locations.')\n",
    "        \n",
    "        position_df_raw = position_df_raw.append(test) \n",
    "    \n",
    "    position_df_raw['Index_ABS'] = np.arange(0,len(position_df_raw))\n",
    "    position_df_raw = position_df_raw.rename_axis(\"Index_Relative\")\n",
    "\n",
    "    # Now remove duplicates\n",
    "    duplicates = position_df_raw[position_df_raw.duplicated(subset='Time') == True]\n",
    "    \n",
    "    position_df_temp = position_df_raw.drop_duplicates(subset=['Time'])\n",
    "    \n",
    "    print(f'Merged dataframe with all platforms. Total of {len(position_df_raw)} measurement positions')\n",
    "    print(f'Duplicates: \\t{len(duplicates)} / {len(position_df_raw)} \\nRemaining: \\t{len(position_df_temp)} / {len(position_df_raw)}')\n",
    "    \n",
    "    return position_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterBBOXandTIME(position_df, time1, time2):\n",
    "    # Filter the position_df dataframe by BBOX\n",
    "    position_df_bbox = position_df[(position_df.loc[:,'Longitude_WGS84'] >= bbox_g[0]) & \n",
    "                                   (position_df.loc[:,'Longitude_WGS84'] <= bbox_g[1]) & \n",
    "                                   (position_df.loc[:,'Latitude_WGS84'] >= bbox_g[2]) & \n",
    "                                   (position_df.loc[:,'Latitude_WGS84'] <= bbox_g[3])]\n",
    "\n",
    "    # Print filtering results on original dataframe\n",
    "    global sel_outof_all\n",
    "    sel_outof_all = f'{len(position_df_bbox)} out of {len(position_df)}.'\n",
    "#     print(f'Selected positions (out of available positions): {sel_outof_all}')\n",
    "#     print(position_df_bbox)\n",
    "\n",
    "    # Filter the position_df_bbox dataframe by TIME\n",
    "    position_df_bbox_timerange = position_df_bbox.loc[(position_df_bbox['Time']>=time_start) & \n",
    "                                                      (position_df_bbox['Time']<=time_end)]\n",
    "\n",
    "    # Print filtering results on original dataframe\n",
    "    print(f'\\nUser-defined Time Range: {time_str}')\n",
    "    sel_outof_all = f'{len(position_df_bbox_timerange)} out of {len(position_df)}.'\n",
    "    print(f'Selected positions (out of available positions): {sel_outof_all}')\n",
    "\n",
    "    # print(position_df_bbox_timerange)\n",
    "    \n",
    "    return position_df_bbox_timerange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndices(df_filtered):\n",
    "    index_dict = {}\n",
    "    \n",
    "    for pc in df_filtered['Platform'].unique():\n",
    "        index_dict[pc] = df_filtered[df_filtered['Platform']==pc].index.tolist()\n",
    "    \n",
    "    return index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractVARsAndDepth(pc_sel, position_dict, pc_dim_dict, url_info, year):  \n",
    "    data_dict = {}\n",
    "    metadata = {}\n",
    "\n",
    "    for pc in pc_sel:\n",
    "        print(pc)\n",
    "        metadata[pc] = {}\n",
    "\n",
    "        v_min = int(float(position_dict[pc]['data_attr'][6]))\n",
    "        metadata[pc]['vmin'] = v_min\n",
    "        metadata[pc]['depth_abs_v1'] = 0 # this is fixed\n",
    "        metadata[pc]['depth_abs_v2'] = pc_dim_dict[pc]['DEPTH'] # this is fixed\n",
    "\n",
    "        # ==============================================================================\n",
    "        \"\"\"\n",
    "        Define here the DEPTH range of your selection, in meters. Note that either:\n",
    "        - 'depth_m_v1' is equal to the lower bound (ie index=0), or \n",
    "        - 'depth_m_v2' is equal to the upper bound (ie index=-1)\n",
    "        \"\"\"\n",
    "        metadata[pc]['depth_m_v1'] = 0 \n",
    "        metadata[pc]['depth_m_v2'] = depth_g\n",
    "        # ==============================================================================\n",
    "\n",
    "        # assert metadata[pc]['depth_m_v1'] < metadata[pc]['depth_m_v2'], 'ERROR: the lower bound must be lower than the higher bound' \n",
    "        # assert metadata[pc]['depth_m_v1'] == 0 or metadata[pc]['depth_m_v2'] == pc_dim_dict[pc]['DEPTH'], 'ERROR: one of the two values must be equal to one of the lower/upper bounds'\n",
    "\n",
    "        #     print(f'DEPTH range of interest (meters): {metadata[pc][\"depth_m_v1\"]} - {metadata[pc][\"depth_m_v2\"]}')\n",
    "\n",
    "        # the start and stop values are adjusted based on the vmin value\n",
    "        if metadata[pc]['vmin'] == 1: \n",
    "            if metadata[pc]['depth_m_v1'] == 0: # \n",
    "                metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1'] # the same\n",
    "                metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2'] # the same, so I have the right size. When I shift and add the nan, I get rid of further element on the right\n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2']# - 1\n",
    "\n",
    "            elif metadata[pc]['depth_m_v1'] != 0: \n",
    "                metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1'] - 1 # start one element before\n",
    "                metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2'] - 1 # last element is excluded, ie stop one element before. But then I'll have to remoove one element\n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2'] - metadata[pc]['depth_m_v1'] - 1 \n",
    "\n",
    "        else:\n",
    "            metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1']\n",
    "            metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2']\n",
    "\n",
    "            if metadata[pc]['depth_m_v1'] == 0: # \n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2']\n",
    "\n",
    "            elif metadata[pc]['depth_m_v1'] != 0: \n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2'] - metadata[pc]['depth_m_v1']\n",
    "\n",
    "        metadata[pc]['depth_newindex4xr_v1'] = 0\n",
    "\n",
    "        pprint.pprint(metadata[pc])\n",
    "        print(f'{pc} DEPTH range of interest (adjusted with vmin): {metadata[pc][\"depth_newindex_v1\"]} - {metadata[pc][\"depth_newindex_v2\"]}')\n",
    "\n",
    "        fix_lab = f'58{pc}_CTD_{year}' # platform_codes and year are defined at the beginning of the notebook \n",
    "\n",
    "        # Get coordinates (needed for keeping the correct structure, and for plotting) \n",
    "        coords_str = getQueryString(pc_dim_dict[pc], keylist = ['TIME', 'LATITUDE', 'LONGITUDE']) # list the coordinates you want\n",
    "\n",
    "        # Extract TIME and DEPTH dimension for queries \n",
    "        time_dims = getQuery(pc, start=0, stop=pc_dim_dict[pc]['TIME'])\n",
    "        depth_dims = getQuery(pc, start=metadata[pc]['depth_newindex_v1'], stop=metadata[pc]['depth_newindex_v2'])#; print(depth_dims)\n",
    "\n",
    "        # join TIME and DEPTH for Variables\n",
    "        var_str_ALL = []\n",
    "        for v in vars_g: var_str_ALL = np.append(var_str_ALL, f'{v}{time_dims}{depth_dims}')\n",
    "        queries_vars = ','.join(var_str_ALL)\n",
    "\n",
    "        # Build url and url with queries (url_q)\n",
    "        url = f'{url_info[0]}{fix_lab}.nc{url_info[1]}?{coords_str}' \n",
    "        url_q = f'{url},{queries_vars}'; print(f'Platform {pc} URL:', url_q)\n",
    "\n",
    "        remote_data, data_attr = fetch_data(url_q, year)\n",
    "\n",
    "        data_dict[pc] = {'data': remote_data, \n",
    "                         'data_attr': data_attr}\n",
    "\n",
    "        print(f'{data_attr}\\n')\n",
    "\n",
    "    assert pc_sel == list(data_dict.keys()), 'ERROR: different platforms, please check.'\n",
    "    \n",
    "    return data_dict, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save Attributes to a database\n",
    "def getAttributes(my_df, my_dict):\n",
    "    \n",
    "    for key in my_dict.keys():\n",
    "\n",
    "        my_df.loc[key,'Platform_code'] = [my_dict[key]['data_attr'][0].astype(str)]\n",
    "        my_df.loc[key,'Platform_name'] = [my_dict[key]['data_attr'][1].astype(str)]\n",
    "        my_df.loc[key,'Year'] = [my_dict[key]['data_attr'][2].astype(int)]\n",
    "        my_df.loc[key,'Data_type'] = [my_dict[key]['data_attr'][3].astype(str)]\n",
    "        my_df.loc[key,'Title'] = [my_dict[key]['data_attr'][4].astype(str)]\n",
    "        my_df.loc[key,'Instrument'] = [my_dict[key]['data_attr'][5].astype(str)]\n",
    "        my_df.loc[key,'Vertical_min'] = [my_dict[key]['data_attr'][6].astype(float)]\n",
    "        my_df.loc[key,'Vertical_max'] = [my_dict[key]['data_attr'][7].astype(float)]\n",
    "        my_df.loc[key,'Lon_min'] = [my_dict[key]['data_attr'][8].astype(float)]\n",
    "        my_df.loc[key,'Lon_max'] = [my_dict[key]['data_attr'][9].astype(float)]\n",
    "        my_df.loc[key,'Lat_min'] = [my_dict[key]['data_attr'][10].astype(float)]\n",
    "        my_df.loc[key,'Lat_max'] = [my_dict[key]['data_attr'][11].astype(float)]\n",
    "\n",
    "    return my_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVminDict(overview_df):\n",
    "    vmin_dict = {}\n",
    "\n",
    "    # select only those platforms where vmin == 1\n",
    "    vmin_pc = overview_df[overview_df['Vertical_min'] == 1.0].index\n",
    "\n",
    "    for i in vmin_pc:\n",
    "        vmin_dict[i] = {}\n",
    "\n",
    "        for v in vars_g:\n",
    "            vmin_dict[i][v] = False\n",
    "    \n",
    "    return vmin_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check whether data should be aligned if vmin = 1, and align if so if has not been done already\n",
    "def check_alignment(data_dict, pc, var, align_and_nan, vmin_dict):\n",
    "    \n",
    "    xarr = data_dict[pc]['data']\n",
    "    xarr_var = xarr[var].data\n",
    "    \n",
    "    vmin = float(xarr.attrs['geospatial_vertical_min'])\n",
    "\n",
    "    if vmin == 0:\n",
    "        print(f'Platform: {pc}; Vertical min: {vmin}; Var: {var}')\n",
    "        \n",
    "    elif vmin==1 and vmin_dict[pc][var]==False and align_and_nan: \n",
    "        # shift to the right and add nan in first position \n",
    "        print(f'Platform: {pc}; Vertical min: {vmin}; Var: {var} --> aligning and add nan')\n",
    "        data_dict[pc]['data'][var].data = adjust_with_vmin(xarr_var, value=np.nan)\n",
    "        vmin_dict[pc][var] = True # to avoid doing hte vmin adjustment for this pc/var more than once        \n",
    "    elif vmin==1 and vmin_dict[pc][var]==False and not align_and_nan: \n",
    "        # No need to shift, this occurred already in the data extraction\n",
    "        print(f'Platform: {pc}; Vertical min: {vmin}; Var: {var} --> data has been aligned already')\n",
    "        vmin_dict[pc][var] = True # to avoid doing hte vmin adjustment for this pc/var more than once\n",
    "    \n",
    "    \n",
    "#     return data_dict[pc], vmin_dict[pc][var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterbyDepthAndIndices(data_dict_yr, metadata_yr, vmin_dict_yr, df_filtered):\n",
    "    \n",
    "    print(f'\\n3) Selected DEPTH: {depth_g}m')\n",
    "    \n",
    "    for year in data_dict_yr.keys():\n",
    "        print(year)\n",
    "        filtered_xarr_dict[year] = {}\n",
    "        \n",
    "        for pc in data_dict_yr[year].keys():\n",
    "            print(pc)\n",
    "            # Generate a filtered xarray with all variables for selected Platform, for a certain DEPTH range\n",
    "            if metadata_yr[year][pc]['depth_m_v1']==0: align_and_nan = True\n",
    "            else: align_and_nan = False\n",
    "\n",
    "            for v in vars_g: \n",
    "                check_alignment(data_dict_yr[year], pc, v, align_and_nan, vmin_dict_yr[year]) \n",
    "\n",
    "            filtered_xarr_dict[year][pc] = filter_xarr_DEPTH(df_filtered, \n",
    "                                                             data_dict_yr[year],\n",
    "                                                             platform=pc,\n",
    "                                                             depth_range=[depth_g, depth_g])\n",
    "            print(filtered_xarr_dict[year][pc])\n",
    "        print('\\n')\n",
    "    \n",
    "    return filtered_xarr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter XARRAY based on platform, Var and DEPTH\n",
    "def filter_xarr_DEPTH(df_toPlot, data_dict, platform, depth_range):\n",
    "    \n",
    "    # find indices for each platform for the selected data\n",
    "    index = df_toPlot[df_toPlot['Platform']==platform].index.tolist()\n",
    "    \n",
    "    # Filer data using the indexes of the filtered elements\n",
    "    xarr_sel = data_dict[platform]['data'].isel(TIME=index,\n",
    "                                                LATITUDE=index,\n",
    "                                                LONGITUDE=index,\n",
    "                                                DEPTH=slice(depth_range[0], depth_range[1]+1))\n",
    "    return xarr_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Range: 20031201-20040131\n",
      "Same Year flag: False\n",
      "Bounding Box: [-20, 40, 50, 85]\n",
      "Depth: 25m\n",
      "Vars: ['TEMP']\n",
      "Output files format(s): ['CSV']\n"
     ]
    }
   ],
   "source": [
    "depth = 25\n",
    "bbox = \"-20, 40, 50, 85\"\n",
    "time1 = '20031201' \n",
    "time2 = '20040131' \n",
    "vars_sel = 'TEMP'\n",
    "formats = 'csv'\n",
    "group = False\n",
    "\n",
    "checkParams(depth, bbox, time1, time2, vars_sel, group, formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server URL and URL suffix: ['http://opendap1.nodc.no/opendap/physics/point/yearly/', '']\n"
     ]
    }
   ],
   "source": [
    "# URL of Norwegian Marine Data Centre (NMDC) data server\n",
    "nmdc_url = 'http://opendap1.nodc.no/opendap/physics/point/yearly/' \n",
    "\n",
    "# URL of Terradue Cloud Platform Hyrax server\n",
    "# Ellip user account and VPN setup required\n",
    "t2_hyrax_url = 'https://opendap.terradue.com/hyrax/data/subset_2003/'\n",
    "\n",
    "urls = {}\n",
    "urls['nmdc'] = [nmdc_url, '']\n",
    "urls['t2_hyrax'] = [t2_hyrax_url, '.nc4']\n",
    "\n",
    "#========================================================\n",
    "# Define below the URL to use (either 'nmdc' or 't2_hyrax'):\n",
    "url_info = urls['nmdc']\n",
    "#========================================================\n",
    "\n",
    "print('Server URL and URL suffix:', url_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of NetCDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on year: 2003\n",
      "{'AA': {'DEPTH': 2809, 'LATITUDE': 683, 'LONGITUDE': 683, 'TIME': 683},\n",
      " 'GS': {'DEPTH': 3683, 'LATITUDE': 404, 'LONGITUDE': 404, 'TIME': 404},\n",
      " 'GT': {'DEPTH': 2956, 'LATITUDE': 990, 'LONGITUDE': 990, 'TIME': 990},\n",
      " 'HJ': {'DEPTH': 789, 'LATITUDE': 178, 'LONGITUDE': 178, 'TIME': 178},\n",
      " 'JH': {'DEPTH': 3763, 'LATITUDE': 949, 'LONGITUDE': 949, 'TIME': 949}}\n",
      "Available platforms in given year 2003: ['AA', 'GS', 'GT', 'HJ', 'JH']\n",
      "Platform: AA. URL with Queries: http://opendap1.nodc.no/opendap/physics/point/yearly/58AA_CTD_2003.nc?TIME[0:1:683],LATITUDE[0:1:683],LONGITUDE[0:1:683]\n",
      "Platform: GS. URL with Queries: http://opendap1.nodc.no/opendap/physics/point/yearly/58GS_CTD_2003.nc?TIME[0:1:404],LATITUDE[0:1:404],LONGITUDE[0:1:404]\n",
      "Platform: GT. URL with Queries: http://opendap1.nodc.no/opendap/physics/point/yearly/58GT_CTD_2003.nc?TIME[0:1:990],LATITUDE[0:1:990],LONGITUDE[0:1:990]\n",
      "Platform: HJ. URL with Queries: http://opendap1.nodc.no/opendap/physics/point/yearly/58HJ_CTD_2003.nc?TIME[0:1:178],LATITUDE[0:1:178],LONGITUDE[0:1:178]\n",
      "Platform: JH. URL with Queries: http://opendap1.nodc.no/opendap/physics/point/yearly/58JH_CTD_2003.nc?TIME[0:1:949],LATITUDE[0:1:949],LONGITUDE[0:1:949]\n",
      "Platform AA: 684 measurement locations.\n",
      "Platform GS: 405 measurement locations.\n",
      "Platform GT: 991 measurement locations.\n",
      "Platform HJ: 179 measurement locations.\n",
      "Platform JH: 950 measurement locations.\n",
      "Merged dataframe with all platforms. Total of 3209 measurement positions\n",
      "Duplicates: \t0 / 3209 \n",
      "Remaining: \t3209 / 3209\n",
      "\n",
      "Working on year: 2004\n",
      "{'AA': {'DEPTH': 2181, 'LATITUDE': 1205, 'LONGITUDE': 1205, 'TIME': 1205},\n",
      " 'GS': {'DEPTH': 4380, 'LATITUDE': 582, 'LONGITUDE': 582, 'TIME': 582},\n",
      " 'HJ': {'DEPTH': 2650, 'LATITUDE': 236, 'LONGITUDE': 236, 'TIME': 236},\n",
      " 'JH': {'DEPTH': 3726, 'LATITUDE': 1144, 'LONGITUDE': 1144, 'TIME': 1144}}\n",
      "Available platforms in given year 2004: ['AA', 'GS', 'HJ', 'JH']\n",
      "Platform: AA. URL with Queries: http://opendap1.nodc.no/opendap/physics/point/yearly/58AA_CTD_2004.nc?TIME[0:1:1205],LATITUDE[0:1:1205],LONGITUDE[0:1:1205]\n",
      "Platform: GS. URL with Queries: http://opendap1.nodc.no/opendap/physics/point/yearly/58GS_CTD_2004.nc?TIME[0:1:582],LATITUDE[0:1:582],LONGITUDE[0:1:582]\n",
      "Platform: HJ. URL with Queries: http://opendap1.nodc.no/opendap/physics/point/yearly/58HJ_CTD_2004.nc?TIME[0:1:236],LATITUDE[0:1:236],LONGITUDE[0:1:236]\n",
      "Platform: JH. URL with Queries: http://opendap1.nodc.no/opendap/physics/point/yearly/58JH_CTD_2004.nc?TIME[0:1:1144],LATITUDE[0:1:1144],LONGITUDE[0:1:1144]\n",
      "Platform AA: 1206 measurement locations.\n",
      "Platform GS: 583 measurement locations.\n",
      "Platform HJ: 237 measurement locations.\n",
      "Platform JH: 1145 measurement locations.\n",
      "Merged dataframe with all platforms. Total of 3171 measurement positions\n",
      "Duplicates: \t0 / 3171 \n",
      "Remaining: \t3171 / 3171\n",
      "COMBINED position_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Longitude_WGS84</th>\n",
       "      <th>Latitude_WGS84</th>\n",
       "      <th>Time</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Index_ABS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.615500</td>\n",
       "      <td>60.755299</td>\n",
       "      <td>2003-01-07 05:25:57</td>\n",
       "      <td>AA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.448300</td>\n",
       "      <td>60.748299</td>\n",
       "      <td>2003-01-07 06:05:08</td>\n",
       "      <td>AA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.285300</td>\n",
       "      <td>60.751499</td>\n",
       "      <td>2003-01-07 08:34:00</td>\n",
       "      <td>AA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.117000</td>\n",
       "      <td>60.747799</td>\n",
       "      <td>2003-01-07 09:18:06</td>\n",
       "      <td>AA</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.947500</td>\n",
       "      <td>60.752201</td>\n",
       "      <td>2003-01-07 09:59:37</td>\n",
       "      <td>AA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6375</th>\n",
       "      <td>16.844000</td>\n",
       "      <td>70.254799</td>\n",
       "      <td>2004-12-08 09:47:35</td>\n",
       "      <td>JH</td>\n",
       "      <td>3166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6376</th>\n",
       "      <td>16.143801</td>\n",
       "      <td>70.095802</td>\n",
       "      <td>2004-12-08 14:45:25</td>\n",
       "      <td>JH</td>\n",
       "      <td>3167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6377</th>\n",
       "      <td>15.987700</td>\n",
       "      <td>71.083801</td>\n",
       "      <td>2004-12-12 23:58:24</td>\n",
       "      <td>JH</td>\n",
       "      <td>3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378</th>\n",
       "      <td>16.045200</td>\n",
       "      <td>71.494698</td>\n",
       "      <td>2004-12-13 12:56:08</td>\n",
       "      <td>JH</td>\n",
       "      <td>3169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6379</th>\n",
       "      <td>14.609300</td>\n",
       "      <td>71.463303</td>\n",
       "      <td>2004-12-17 11:21:05</td>\n",
       "      <td>JH</td>\n",
       "      <td>3170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6380 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Longitude_WGS84  Latitude_WGS84                Time Platform  Index_ABS\n",
       "0            4.615500       60.755299 2003-01-07 05:25:57       AA          0\n",
       "1            4.448300       60.748299 2003-01-07 06:05:08       AA          1\n",
       "2            4.285300       60.751499 2003-01-07 08:34:00       AA          2\n",
       "3            4.117000       60.747799 2003-01-07 09:18:06       AA          3\n",
       "4            3.947500       60.752201 2003-01-07 09:59:37       AA          4\n",
       "...               ...             ...                 ...      ...        ...\n",
       "6375        16.844000       70.254799 2004-12-08 09:47:35       JH       3166\n",
       "6376        16.143801       70.095802 2004-12-08 14:45:25       JH       3167\n",
       "6377        15.987700       71.083801 2004-12-12 23:58:24       JH       3168\n",
       "6378        16.045200       71.494698 2004-12-13 12:56:08       JH       3169\n",
       "6379        14.609300       71.463303 2004-12-17 11:21:05       JH       3170\n",
       "\n",
       "[6380 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dds_year_dict = {}\n",
    "pos_year_dict = {}\n",
    "global position_df\n",
    "position_df = pd.DataFrame()\n",
    "\n",
    "for year in range(year1, year2+1): # need to do a for loop over the years as the data is saved in years on the server\n",
    "    print('\\nWorking on year:', year)\n",
    "    \n",
    "    # Retrieval of DDS info\n",
    "    dds_year_dict[year] = getDDS(url_info, year) # dds_year_dict[year] replaced pc_dim_dict \n",
    "    pprint.pprint(dds_year_dict[year])\n",
    "    \n",
    "    # Extract all platform_codes for that year\n",
    "    platform_codes = [pc for pc in dds_year_dict[year].keys()]\n",
    "    print(f'Available platforms in given year {year}: {platform_codes}')\n",
    "\n",
    "    # Create position_dict\n",
    "    pos_year_dict[year] = getPositionDict(dds_year_dict[year], url_info, year) # pos_year_dict[year] replaced position_dict\n",
    "#     pprint.pprint(pos_year_dict[year])\n",
    "\n",
    "    # Match and merge LAT, LONG and TIME of positions in a position_df dataframe\n",
    "    position_df_temp = makePositionDF(pos_year_dict[year])\n",
    "    position_df = position_df.append(position_df_temp, ignore_index=True)\n",
    "\n",
    "print('COMBINED position_df')\n",
    "display(position_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by BBOX and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User-defined Time Range: 20031201-20040131\n",
      "Selected positions (out of available positions): 162 out of 6380.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Longitude_WGS84</th>\n",
       "      <th>Latitude_WGS84</th>\n",
       "      <th>Time</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Index_ABS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>5.098700</td>\n",
       "      <td>61.916801</td>\n",
       "      <td>2003-12-01 06:06:42</td>\n",
       "      <td>AA</td>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>5.350700</td>\n",
       "      <td>62.132198</td>\n",
       "      <td>2003-12-01 15:04:22</td>\n",
       "      <td>AA</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>5.458200</td>\n",
       "      <td>62.091702</td>\n",
       "      <td>2003-12-01 16:25:02</td>\n",
       "      <td>AA</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>5.511800</td>\n",
       "      <td>62.060001</td>\n",
       "      <td>2003-12-01 17:00:58</td>\n",
       "      <td>AA</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>5.500700</td>\n",
       "      <td>62.035301</td>\n",
       "      <td>2003-12-01 17:28:26</td>\n",
       "      <td>AA</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5294</th>\n",
       "      <td>31.205500</td>\n",
       "      <td>71.252296</td>\n",
       "      <td>2004-01-27 00:42:36</td>\n",
       "      <td>JH</td>\n",
       "      <td>2085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5295</th>\n",
       "      <td>31.212500</td>\n",
       "      <td>71.005798</td>\n",
       "      <td>2004-01-27 02:46:14</td>\n",
       "      <td>JH</td>\n",
       "      <td>2086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5296</th>\n",
       "      <td>31.215300</td>\n",
       "      <td>70.751701</td>\n",
       "      <td>2004-01-27 04:33:22</td>\n",
       "      <td>JH</td>\n",
       "      <td>2087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5297</th>\n",
       "      <td>31.217501</td>\n",
       "      <td>70.498703</td>\n",
       "      <td>2004-01-27 06:34:35</td>\n",
       "      <td>JH</td>\n",
       "      <td>2088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5298</th>\n",
       "      <td>31.217199</td>\n",
       "      <td>70.397301</td>\n",
       "      <td>2004-01-27 07:22:52</td>\n",
       "      <td>JH</td>\n",
       "      <td>2089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Longitude_WGS84  Latitude_WGS84                Time Platform  Index_ABS\n",
       "647          5.098700       61.916801 2003-12-01 06:06:42       AA        647\n",
       "648          5.350700       62.132198 2003-12-01 15:04:22       AA        648\n",
       "649          5.458200       62.091702 2003-12-01 16:25:02       AA        649\n",
       "650          5.511800       62.060001 2003-12-01 17:00:58       AA        650\n",
       "651          5.500700       62.035301 2003-12-01 17:28:26       AA        651\n",
       "...               ...             ...                 ...      ...        ...\n",
       "5294        31.205500       71.252296 2004-01-27 00:42:36       JH       2085\n",
       "5295        31.212500       71.005798 2004-01-27 02:46:14       JH       2086\n",
       "5296        31.215300       70.751701 2004-01-27 04:33:22       JH       2087\n",
       "5297        31.217501       70.498703 2004-01-27 06:34:35       JH       2088\n",
       "5298        31.217199       70.397301 2004-01-27 07:22:52       JH       2089\n",
       "\n",
       "[162 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AA': [647,\n",
      "        648,\n",
      "        649,\n",
      "        650,\n",
      "        651,\n",
      "        652,\n",
      "        653,\n",
      "        654,\n",
      "        655,\n",
      "        656,\n",
      "        657,\n",
      "        658,\n",
      "        659,\n",
      "        660,\n",
      "        661,\n",
      "        662,\n",
      "        663,\n",
      "        664,\n",
      "        665,\n",
      "        666,\n",
      "        667,\n",
      "        668,\n",
      "        669,\n",
      "        670,\n",
      "        671,\n",
      "        672,\n",
      "        673,\n",
      "        674,\n",
      "        675,\n",
      "        676,\n",
      "        677,\n",
      "        678,\n",
      "        679,\n",
      "        680,\n",
      "        681,\n",
      "        682,\n",
      "        683],\n",
      " 'GS': [1063,\n",
      "        1064,\n",
      "        1065,\n",
      "        1066,\n",
      "        1067,\n",
      "        1068,\n",
      "        1069,\n",
      "        1070,\n",
      "        1071,\n",
      "        1072,\n",
      "        1073,\n",
      "        1074,\n",
      "        1075,\n",
      "        1076,\n",
      "        1077,\n",
      "        1078,\n",
      "        1079,\n",
      "        1080,\n",
      "        1081,\n",
      "        1082,\n",
      "        1083,\n",
      "        1084,\n",
      "        1085,\n",
      "        1086,\n",
      "        1087,\n",
      "        1088,\n",
      "        4415,\n",
      "        4416,\n",
      "        4417,\n",
      "        4418,\n",
      "        4419,\n",
      "        4420,\n",
      "        4421,\n",
      "        4422,\n",
      "        4423,\n",
      "        4424],\n",
      " 'JH': [3184,\n",
      "        3185,\n",
      "        3186,\n",
      "        3187,\n",
      "        3188,\n",
      "        3189,\n",
      "        3190,\n",
      "        3191,\n",
      "        3192,\n",
      "        3193,\n",
      "        3194,\n",
      "        3195,\n",
      "        3196,\n",
      "        3197,\n",
      "        3198,\n",
      "        3199,\n",
      "        3200,\n",
      "        3201,\n",
      "        3202,\n",
      "        3203,\n",
      "        3204,\n",
      "        3205,\n",
      "        3206,\n",
      "        3207,\n",
      "        3208,\n",
      "        5235,\n",
      "        5236,\n",
      "        5237,\n",
      "        5238,\n",
      "        5239,\n",
      "        5240,\n",
      "        5241,\n",
      "        5242,\n",
      "        5243,\n",
      "        5244,\n",
      "        5245,\n",
      "        5246,\n",
      "        5247,\n",
      "        5248,\n",
      "        5249,\n",
      "        5250,\n",
      "        5251,\n",
      "        5252,\n",
      "        5253,\n",
      "        5254,\n",
      "        5255,\n",
      "        5256,\n",
      "        5257,\n",
      "        5258,\n",
      "        5259,\n",
      "        5260,\n",
      "        5261,\n",
      "        5262,\n",
      "        5263,\n",
      "        5264,\n",
      "        5265,\n",
      "        5266,\n",
      "        5267,\n",
      "        5268,\n",
      "        5269,\n",
      "        5270,\n",
      "        5271,\n",
      "        5272,\n",
      "        5273,\n",
      "        5274,\n",
      "        5275,\n",
      "        5276,\n",
      "        5277,\n",
      "        5278,\n",
      "        5279,\n",
      "        5280,\n",
      "        5281,\n",
      "        5282,\n",
      "        5283,\n",
      "        5284,\n",
      "        5285,\n",
      "        5286,\n",
      "        5287,\n",
      "        5288,\n",
      "        5289,\n",
      "        5290,\n",
      "        5291,\n",
      "        5292,\n",
      "        5293,\n",
      "        5294,\n",
      "        5295,\n",
      "        5296,\n",
      "        5297,\n",
      "        5298]}\n"
     ]
    }
   ],
   "source": [
    "# Filter by BBOX and Time\n",
    "df_filtered = filterBBOXandTIME(position_df, time1, time2)\n",
    "display(df_filtered)\n",
    "\n",
    " # Dictionary of indices\n",
    "index_dict = getIndices(df_filtered)\n",
    "pprint.pprint(index_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing: Load and Plot selected Data (Variables within DEPTH range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on year: 2003 - Available platforms: ['AA', 'GS', 'GT', 'HJ', 'JH']\n",
      "AA\n",
      "{'depth_abs_v1': 0,\n",
      " 'depth_abs_v2': 2809,\n",
      " 'depth_m_v1': 0,\n",
      " 'depth_m_v2': 25,\n",
      " 'depth_newindex4xr_v1': 0,\n",
      " 'depth_newindex4xr_v2': 25,\n",
      " 'depth_newindex_v1': 0,\n",
      " 'depth_newindex_v2': 25,\n",
      " 'vmin': 0}\n",
      "AA DEPTH range of interest (adjusted with vmin): 0 - 25\n",
      "Platform AA URL: http://opendap1.nodc.no/opendap/physics/point/yearly/58AA_CTD_2003.nc?TIME[0:1:683],LATITUDE[0:1:683],LONGITUDE[0:1:683],TEMP[0:1:683][0:1:25]\n",
      "['58AA' 'H\\\\303\\\\245kon Mosby' '2003' 'OceanSITES vertical profile'\n",
      " 'Arctic Ocean - In Situ Observation Copernicus' 'CTD Seabird' '0.000'\n",
      " '2811.000' '-11.884' '17.833' '53.996' '69.999']\n",
      "\n",
      "GS\n",
      "{'depth_abs_v1': 0,\n",
      " 'depth_abs_v2': 3683,\n",
      " 'depth_m_v1': 0,\n",
      " 'depth_m_v2': 25,\n",
      " 'depth_newindex4xr_v1': 0,\n",
      " 'depth_newindex4xr_v2': 25,\n",
      " 'depth_newindex_v1': 0,\n",
      " 'depth_newindex_v2': 25,\n",
      " 'vmin': 1}\n",
      "GS DEPTH range of interest (adjusted with vmin): 0 - 25\n",
      "Platform GS URL: http://opendap1.nodc.no/opendap/physics/point/yearly/58GS_CTD_2003.nc?TIME[0:1:404],LATITUDE[0:1:404],LONGITUDE[0:1:404],TEMP[0:1:404][0:1:25]\n",
      "['58GS' 'G.O. Sars' '2003' 'OceanSITES vertical profile'\n",
      " 'Arctic Ocean - In Situ Observation Copernicus' 'CTD Seabird' '1.000'\n",
      " '3688.000' '-17.979' '22.532' '59.28' '78.336']\n",
      "\n",
      "GT\n",
      "{'depth_abs_v1': 0,\n",
      " 'depth_abs_v2': 2956,\n",
      " 'depth_m_v1': 0,\n",
      " 'depth_m_v2': 25,\n",
      " 'depth_newindex4xr_v1': 0,\n",
      " 'depth_newindex4xr_v2': 25,\n",
      " 'depth_newindex_v1': 0,\n",
      " 'depth_newindex_v2': 25,\n",
      " 'vmin': 1}\n",
      "GT DEPTH range of interest (adjusted with vmin): 0 - 25\n",
      "Platform GT URL: http://opendap1.nodc.no/opendap/physics/point/yearly/58GT_CTD_2003.nc?TIME[0:1:990],LATITUDE[0:1:990],LONGITUDE[0:1:990],TEMP[0:1:990][0:1:25]\n",
      "['58GT' 'Sarsen' '2003' 'OceanSITES vertical profile'\n",
      " 'Arctic Ocean - In Situ Observation Copernicus' 'CTD Seabird' '1.000'\n",
      " '2962.000' '-9.52' '35.016' '56.667' '78.655']\n",
      "\n",
      "HJ\n",
      "{'depth_abs_v1': 0,\n",
      " 'depth_abs_v2': 789,\n",
      " 'depth_m_v1': 0,\n",
      " 'depth_m_v2': 25,\n",
      " 'depth_newindex4xr_v1': 0,\n",
      " 'depth_newindex4xr_v2': 25,\n",
      " 'depth_newindex_v1': 0,\n",
      " 'depth_newindex_v2': 25,\n",
      " 'vmin': 1}\n",
      "HJ DEPTH range of interest (adjusted with vmin): 0 - 25\n",
      "Platform HJ URL: http://opendap1.nodc.no/opendap/physics/point/yearly/58HJ_CTD_2003.nc?TIME[0:1:178],LATITUDE[0:1:178],LONGITUDE[0:1:178],TEMP[0:1:178][0:1:25]\n",
      "['58HJ' 'Helmer Hanssen' '2003' 'OceanSITES vertical profile'\n",
      " 'Arctic Ocean - In Situ Observation Copernicus' 'CTD Seabird' '1.000'\n",
      " '792.000' '6.303' '39.826' '63.889' '81.179']\n",
      "\n",
      "JH\n",
      "{'depth_abs_v1': 0,\n",
      " 'depth_abs_v2': 3763,\n",
      " 'depth_m_v1': 0,\n",
      " 'depth_m_v2': 25,\n",
      " 'depth_newindex4xr_v1': 0,\n",
      " 'depth_newindex4xr_v2': 25,\n",
      " 'depth_newindex_v1': 0,\n",
      " 'depth_newindex_v2': 25,\n",
      " 'vmin': 1}\n",
      "JH DEPTH range of interest (adjusted with vmin): 0 - 25\n",
      "Platform JH URL: http://opendap1.nodc.no/opendap/physics/point/yearly/58JH_CTD_2003.nc?TIME[0:1:949],LATITUDE[0:1:949],LONGITUDE[0:1:949],TEMP[0:1:949][0:1:25]\n",
      "['58JH' 'Johan Hjort' '2003' 'OceanSITES vertical profile'\n",
      " 'Arctic Ocean - In Situ Observation Copernicus' 'CTD Seabird' '1.000'\n",
      " '3766.000' '-15.502' '47.017' '50.998' '77.829']\n",
      "\n",
      "Attributes Year: 2003\n",
      "overview_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform_code</th>\n",
       "      <th>Platform_name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Data_type</th>\n",
       "      <th>Title</th>\n",
       "      <th>Instrument</th>\n",
       "      <th>Vertical_min</th>\n",
       "      <th>Vertical_max</th>\n",
       "      <th>Lon_min</th>\n",
       "      <th>Lon_max</th>\n",
       "      <th>Lat_min</th>\n",
       "      <th>Lat_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AA</th>\n",
       "      <td>58AA</td>\n",
       "      <td>H\\303\\245kon Mosby</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>OceanSITES vertical profile</td>\n",
       "      <td>Arctic Ocean - In Situ Observation Copernicus</td>\n",
       "      <td>CTD Seabird</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2811.0</td>\n",
       "      <td>-11.884</td>\n",
       "      <td>17.833</td>\n",
       "      <td>53.996</td>\n",
       "      <td>69.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GS</th>\n",
       "      <td>58GS</td>\n",
       "      <td>G.O. Sars</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>OceanSITES vertical profile</td>\n",
       "      <td>Arctic Ocean - In Situ Observation Copernicus</td>\n",
       "      <td>CTD Seabird</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3688.0</td>\n",
       "      <td>-17.979</td>\n",
       "      <td>22.532</td>\n",
       "      <td>59.280</td>\n",
       "      <td>78.336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GT</th>\n",
       "      <td>58GT</td>\n",
       "      <td>Sarsen</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>OceanSITES vertical profile</td>\n",
       "      <td>Arctic Ocean - In Situ Observation Copernicus</td>\n",
       "      <td>CTD Seabird</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2962.0</td>\n",
       "      <td>-9.520</td>\n",
       "      <td>35.016</td>\n",
       "      <td>56.667</td>\n",
       "      <td>78.655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HJ</th>\n",
       "      <td>58HJ</td>\n",
       "      <td>Helmer Hanssen</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>OceanSITES vertical profile</td>\n",
       "      <td>Arctic Ocean - In Situ Observation Copernicus</td>\n",
       "      <td>CTD Seabird</td>\n",
       "      <td>1.0</td>\n",
       "      <td>792.0</td>\n",
       "      <td>6.303</td>\n",
       "      <td>39.826</td>\n",
       "      <td>63.889</td>\n",
       "      <td>81.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JH</th>\n",
       "      <td>58JH</td>\n",
       "      <td>Johan Hjort</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>OceanSITES vertical profile</td>\n",
       "      <td>Arctic Ocean - In Situ Observation Copernicus</td>\n",
       "      <td>CTD Seabird</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3766.0</td>\n",
       "      <td>-15.502</td>\n",
       "      <td>47.017</td>\n",
       "      <td>50.998</td>\n",
       "      <td>77.829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Platform_code       Platform_name    Year                    Data_type  \\\n",
       "AA          58AA  H\\303\\245kon Mosby  2003.0  OceanSITES vertical profile   \n",
       "GS          58GS           G.O. Sars  2003.0  OceanSITES vertical profile   \n",
       "GT          58GT              Sarsen  2003.0  OceanSITES vertical profile   \n",
       "HJ          58HJ      Helmer Hanssen  2003.0  OceanSITES vertical profile   \n",
       "JH          58JH         Johan Hjort  2003.0  OceanSITES vertical profile   \n",
       "\n",
       "                                            Title   Instrument  Vertical_min  \\\n",
       "AA  Arctic Ocean - In Situ Observation Copernicus  CTD Seabird           0.0   \n",
       "GS  Arctic Ocean - In Situ Observation Copernicus  CTD Seabird           1.0   \n",
       "GT  Arctic Ocean - In Situ Observation Copernicus  CTD Seabird           1.0   \n",
       "HJ  Arctic Ocean - In Situ Observation Copernicus  CTD Seabird           1.0   \n",
       "JH  Arctic Ocean - In Situ Observation Copernicus  CTD Seabird           1.0   \n",
       "\n",
       "    Vertical_max  Lon_min  Lon_max  Lat_min  Lat_max  \n",
       "AA        2811.0  -11.884   17.833   53.996   69.999  \n",
       "GS        3688.0  -17.979   22.532   59.280   78.336  \n",
       "GT        2962.0   -9.520   35.016   56.667   78.655  \n",
       "HJ         792.0    6.303   39.826   63.889   81.179  \n",
       "JH        3766.0  -15.502   47.017   50.998   77.829  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on year: 2004 - Available platforms: ['AA', 'GS', 'HJ', 'JH']\n",
      "AA\n",
      "{'depth_abs_v1': 0,\n",
      " 'depth_abs_v2': 2181,\n",
      " 'depth_m_v1': 0,\n",
      " 'depth_m_v2': 25,\n",
      " 'depth_newindex4xr_v1': 0,\n",
      " 'depth_newindex4xr_v2': 25,\n",
      " 'depth_newindex_v1': 0,\n",
      " 'depth_newindex_v2': 25,\n",
      " 'vmin': 1}\n",
      "AA DEPTH range of interest (adjusted with vmin): 0 - 25\n",
      "Platform AA URL: http://opendap1.nodc.no/opendap/physics/point/yearly/58AA_CTD_2004.nc?TIME[0:1:1205],LATITUDE[0:1:1205],LONGITUDE[0:1:1205],TEMP[0:1:1205][0:1:25]\n",
      "['58AA' 'H\\\\303\\\\245kon Mosby' '2004' 'OceanSITES vertical profile'\n",
      " 'Arctic Ocean - In Situ Observation Copernicus'\n",
      " 'CTD Seabird Sonder uspes.; CTD Seabird' '1.000' '2183.000' '-23.754'\n",
      " '35.047' '56.643' '81.306']\n",
      "\n",
      "GS\n",
      "{'depth_abs_v1': 0,\n",
      " 'depth_abs_v2': 4380,\n",
      " 'depth_m_v1': 0,\n",
      " 'depth_m_v2': 25,\n",
      " 'depth_newindex4xr_v1': 0,\n",
      " 'depth_newindex4xr_v2': 25,\n",
      " 'depth_newindex_v1': 0,\n",
      " 'depth_newindex_v2': 25,\n",
      " 'vmin': 1}\n",
      "GS DEPTH range of interest (adjusted with vmin): 0 - 25\n",
      "Platform GS URL: http://opendap1.nodc.no/opendap/physics/point/yearly/58GS_CTD_2004.nc?TIME[0:1:582],LATITUDE[0:1:582],LONGITUDE[0:1:582],TEMP[0:1:582][0:1:25]\n",
      "['58GS' 'G.O. Sars' '2004' 'OceanSITES vertical profile'\n",
      " 'Arctic Ocean - In Situ Observation Copernicus' 'CTD Seabird' '1.000'\n",
      " '4383.000' '-36.811' '36.426' '41.462' '78.44']\n",
      "\n",
      "HJ\n",
      "{'depth_abs_v1': 0,\n",
      " 'depth_abs_v2': 2650,\n",
      " 'depth_m_v1': 0,\n",
      " 'depth_m_v2': 25,\n",
      " 'depth_newindex4xr_v1': 0,\n",
      " 'depth_newindex4xr_v2': 25,\n",
      " 'depth_newindex_v1': 0,\n",
      " 'depth_newindex_v2': 25,\n",
      " 'vmin': 1}\n",
      "HJ DEPTH range of interest (adjusted with vmin): 0 - 25\n",
      "Platform HJ URL: http://opendap1.nodc.no/opendap/physics/point/yearly/58HJ_CTD_2004.nc?TIME[0:1:236],LATITUDE[0:1:236],LONGITUDE[0:1:236],TEMP[0:1:236][0:1:25]\n",
      "['58HJ' 'Helmer Hanssen' '2004' 'OceanSITES vertical profile'\n",
      " 'Arctic Ocean - In Situ Observation Copernicus' 'CTD Seabird' '1.000'\n",
      " '2654.000' '2.974' '36.26' '63.258' '81.874']\n",
      "\n",
      "JH\n",
      "{'depth_abs_v1': 0,\n",
      " 'depth_abs_v2': 3726,\n",
      " 'depth_m_v1': 0,\n",
      " 'depth_m_v2': 25,\n",
      " 'depth_newindex4xr_v1': 0,\n",
      " 'depth_newindex4xr_v2': 25,\n",
      " 'depth_newindex_v1': 0,\n",
      " 'depth_newindex_v2': 25,\n",
      " 'vmin': 1}\n",
      "JH DEPTH range of interest (adjusted with vmin): 0 - 25\n",
      "Platform JH URL: http://opendap1.nodc.no/opendap/physics/point/yearly/58JH_CTD_2004.nc?TIME[0:1:1144],LATITUDE[0:1:1144],LONGITUDE[0:1:1144],TEMP[0:1:1144][0:1:25]\n",
      "['58JH' 'Johan Hjort' '2004' 'OceanSITES vertical profile'\n",
      " 'Arctic Ocean - In Situ Observation Copernicus'\n",
      " 'CTD Seabird CTD Seabird; Sonder uspes.' '1.000' '3730.000' '-16.993'\n",
      " '49.063' '52.244' '80.503']\n",
      "\n",
      "Attributes Year: 2004\n",
      "overview_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform_code</th>\n",
       "      <th>Platform_name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Data_type</th>\n",
       "      <th>Title</th>\n",
       "      <th>Instrument</th>\n",
       "      <th>Vertical_min</th>\n",
       "      <th>Vertical_max</th>\n",
       "      <th>Lon_min</th>\n",
       "      <th>Lon_max</th>\n",
       "      <th>Lat_min</th>\n",
       "      <th>Lat_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AA</th>\n",
       "      <td>58AA</td>\n",
       "      <td>H\\303\\245kon Mosby</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>OceanSITES vertical profile</td>\n",
       "      <td>Arctic Ocean - In Situ Observation Copernicus</td>\n",
       "      <td>CTD Seabird Sonder uspes.; CTD Seabird</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2183.0</td>\n",
       "      <td>-23.754</td>\n",
       "      <td>35.047</td>\n",
       "      <td>56.643</td>\n",
       "      <td>81.306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GS</th>\n",
       "      <td>58GS</td>\n",
       "      <td>G.O. Sars</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>OceanSITES vertical profile</td>\n",
       "      <td>Arctic Ocean - In Situ Observation Copernicus</td>\n",
       "      <td>CTD Seabird</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4383.0</td>\n",
       "      <td>-36.811</td>\n",
       "      <td>36.426</td>\n",
       "      <td>41.462</td>\n",
       "      <td>78.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HJ</th>\n",
       "      <td>58HJ</td>\n",
       "      <td>Helmer Hanssen</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>OceanSITES vertical profile</td>\n",
       "      <td>Arctic Ocean - In Situ Observation Copernicus</td>\n",
       "      <td>CTD Seabird</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2654.0</td>\n",
       "      <td>2.974</td>\n",
       "      <td>36.260</td>\n",
       "      <td>63.258</td>\n",
       "      <td>81.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JH</th>\n",
       "      <td>58JH</td>\n",
       "      <td>Johan Hjort</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>OceanSITES vertical profile</td>\n",
       "      <td>Arctic Ocean - In Situ Observation Copernicus</td>\n",
       "      <td>CTD Seabird CTD Seabird; Sonder uspes.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3730.0</td>\n",
       "      <td>-16.993</td>\n",
       "      <td>49.063</td>\n",
       "      <td>52.244</td>\n",
       "      <td>80.503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Platform_code       Platform_name    Year                    Data_type  \\\n",
       "AA          58AA  H\\303\\245kon Mosby  2004.0  OceanSITES vertical profile   \n",
       "GS          58GS           G.O. Sars  2004.0  OceanSITES vertical profile   \n",
       "HJ          58HJ      Helmer Hanssen  2004.0  OceanSITES vertical profile   \n",
       "JH          58JH         Johan Hjort  2004.0  OceanSITES vertical profile   \n",
       "\n",
       "                                            Title  \\\n",
       "AA  Arctic Ocean - In Situ Observation Copernicus   \n",
       "GS  Arctic Ocean - In Situ Observation Copernicus   \n",
       "HJ  Arctic Ocean - In Situ Observation Copernicus   \n",
       "JH  Arctic Ocean - In Situ Observation Copernicus   \n",
       "\n",
       "                                Instrument  Vertical_min  Vertical_max  \\\n",
       "AA  CTD Seabird Sonder uspes.; CTD Seabird           1.0        2183.0   \n",
       "GS                             CTD Seabird           1.0        4383.0   \n",
       "HJ                             CTD Seabird           1.0        2654.0   \n",
       "JH  CTD Seabird CTD Seabird; Sonder uspes.           1.0        3730.0   \n",
       "\n",
       "    Lon_min  Lon_max  Lat_min  Lat_max  \n",
       "AA  -23.754   35.047   56.643   81.306  \n",
       "GS  -36.811   36.426   41.462   78.440  \n",
       "HJ    2.974   36.260   63.258   81.874  \n",
       "JH  -16.993   49.063   52.244   80.503  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2) ===================Printing Results:\n",
      "\n",
      "2.1) data_dict_yr\n",
      "{2003: {'AA': {'data': <xarray.Dataset>\n",
      "Dimensions:    (TIME: 684, LATITUDE: 684, LONGITUDE: 684, DEPTH: 26)\n",
      "Coordinates:\n",
      "  * TIME       (TIME) float64 1.936e+04 1.936e+04 ... 1.97e+04 1.97e+04\n",
      "  * LATITUDE   (LATITUDE) float32 60.76 60.75 60.75 60.75 ... 63.53 63.59 63.92\n",
      "  * LONGITUDE  (LONGITUDE) float32 4.615 4.448 4.285 4.117 ... 10.36 10.9 11.04\n",
      "Dimensions without coordinates: DEPTH\n",
      "Data variables:\n",
      "    TEMP       (TIME, DEPTH) float64 ...\n",
      "Attributes: (12/152)\n",
      "    _NCProperties:                  version=1|netcdflibversion=4.4.1|hdf5libv...\n",
      "    title:                          Arctic Ocean - In Situ Observation Copern...\n",
      "    qc_manual:                      Recommendations for in-situ data Near Rea...\n",
      "    contact:                        cmems-service@imr.no\n",
      "    format_version:                 1.4\n",
      "    distribution_statement:         These data follow Copernicus standards; t...\n",
      "    ...                             ...\n",
      "    CNDC_DM._FillValue:              \n",
      "    CNDC_DM.conventions:            Copernicus Marine In Situ reference table 1\n",
      "    CNDC_DM.flag_values:            R, A, D\n",
      "    CNDC_DM.flag_meanings:          real-time adjusted-in-real-time delayed-mode\n",
      "    CNDC_DM.long_name:              Electrical conductivity method of data pr...\n",
      "    CNDC_DM.string_length:          2810,\n",
      "               'data_attr': array(['58AA', 'H\\\\303\\\\245kon Mosby', '2003',\n",
      "       'OceanSITES vertical profile',\n",
      "       'Arctic Ocean - In Situ Observation Copernicus', 'CTD Seabird',\n",
      "       '0.000', '2811.000', '-11.884', '17.833', '53.996', '69.999'],\n",
      "      dtype='<U45')},\n",
      "        'GS': {'data': <xarray.Dataset>\n",
      "Dimensions:    (TIME: 405, LATITUDE: 405, LONGITUDE: 405, DEPTH: 26)\n",
      "Coordinates:\n",
      "  * TIME       (TIME) float64 1.956e+04 1.957e+04 ... 1.97e+04 1.97e+04\n",
      "  * LATITUDE   (LATITUDE) float32 70.51 70.49 70.51 70.5 ... 68.37 68.38 68.38\n",
      "  * LONGITUDE  (LONGITUDE) float32 17.95 14.0 9.883 8.01 ... 16.09 16.09 16.09\n",
      "Dimensions without coordinates: DEPTH\n",
      "Data variables:\n",
      "    TEMP       (TIME, DEPTH) float64 ...\n",
      "Attributes: (12/128)\n",
      "    _NCProperties:                  version=1|netcdflibversion=4.4.1|hdf5libv...\n",
      "    title:                          Arctic Ocean - In Situ Observation Copern...\n",
      "    qc_manual:                      Recommendations for in-situ data Near Rea...\n",
      "    contact:                        cmems-service@imr.no\n",
      "    format_version:                 1.4\n",
      "    distribution_statement:         These data follow Copernicus standards; t...\n",
      "    ...                             ...\n",
      "    CNDC_QC.conventions:            Copernicus Marine In Situ reference table 2\n",
      "    CNDC_QC.valid_min:              0\n",
      "    CNDC_QC.valid_max:              9\n",
      "    CNDC_QC.flag_values:            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "    CNDC_QC.flag_meanings:          no_qc_performed good_data probably_good_d...\n",
      "    CNDC_QC.long_name:              Electrical conductivity quality flag,\n",
      "               'data_attr': array(['58GS', 'G.O. Sars', '2003', 'OceanSITES vertical profile',\n",
      "       'Arctic Ocean - In Situ Observation Copernicus', 'CTD Seabird',\n",
      "       '1.000', '3688.000', '-17.979', '22.532', '59.28', '78.336'],\n",
      "      dtype='<U45')},\n",
      "        'GT': {'data': <xarray.Dataset>\n",
      "Dimensions:    (TIME: 991, LATITUDE: 991, LONGITUDE: 991, DEPTH: 26)\n",
      "Coordinates:\n",
      "  * TIME       (TIME) float64 1.938e+04 1.938e+04 ... 1.963e+04 1.964e+04\n",
      "  * LATITUDE   (LATITUDE) float32 62.37 62.49 62.6 62.72 ... 56.74 57.13 57.69\n",
      "  * LONGITUDE  (LONGITUDE) float32 5.2 4.948 4.7 ... -0.3618 -0.5513 -0.5153\n",
      "Dimensions without coordinates: DEPTH\n",
      "Data variables:\n",
      "    TEMP       (TIME, DEPTH) float64 ...\n",
      "Attributes: (12/128)\n",
      "    _NCProperties:                  version=1|netcdflibversion=4.4.1|hdf5libv...\n",
      "    title:                          Arctic Ocean - In Situ Observation Copern...\n",
      "    qc_manual:                      Recommendations for in-situ data Near Rea...\n",
      "    contact:                        cmems-service@imr.no\n",
      "    format_version:                 1.4\n",
      "    distribution_statement:         These data follow Copernicus standards; t...\n",
      "    ...                             ...\n",
      "    CNDC_QC.conventions:            Copernicus Marine In Situ reference table 2\n",
      "    CNDC_QC.valid_min:              0\n",
      "    CNDC_QC.valid_max:              9\n",
      "    CNDC_QC.flag_values:            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "    CNDC_QC.flag_meanings:          no_qc_performed good_data probably_good_d...\n",
      "    CNDC_QC.long_name:              Electrical conductivity quality flag,\n",
      "               'data_attr': array(['58GT', 'Sarsen', '2003', 'OceanSITES vertical profile',\n",
      "       'Arctic Ocean - In Situ Observation Copernicus', 'CTD Seabird',\n",
      "       '1.000', '2962.000', '-9.52', '35.016', '56.667', '78.655'],\n",
      "      dtype='<U45')},\n",
      "        'HJ': {'data': <xarray.Dataset>\n",
      "Dimensions:    (TIME: 179, LATITUDE: 179, LONGITUDE: 179, DEPTH: 26)\n",
      "Coordinates:\n",
      "  * TIME       (TIME) float64 1.96e+04 1.96e+04 1.96e+04 ... 1.968e+04 1.968e+04\n",
      "  * LATITUDE   (LATITUDE) float32 80.02 80.38 80.03 80.02 ... 68.06 68.18 68.23\n",
      "  * LONGITUDE  (LONGITUDE) float32 6.303 8.006 8.285 9.16 ... 10.49 11.05 12.42\n",
      "Dimensions without coordinates: DEPTH\n",
      "Data variables:\n",
      "    TEMP       (TIME, DEPTH) float64 ...\n",
      "Attributes: (12/128)\n",
      "    _NCProperties:                  version=1|netcdflibversion=4.4.1|hdf5libv...\n",
      "    title:                          Arctic Ocean - In Situ Observation Copern...\n",
      "    qc_manual:                      Recommendations for in-situ data Near Rea...\n",
      "    contact:                        cmems-service@imr.no\n",
      "    format_version:                 1.4\n",
      "    distribution_statement:         These data follow Copernicus standards; t...\n",
      "    ...                             ...\n",
      "    CNDC_QC.conventions:            Copernicus Marine In Situ reference table 2\n",
      "    CNDC_QC.valid_min:              0\n",
      "    CNDC_QC.valid_max:              9\n",
      "    CNDC_QC.flag_values:            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "    CNDC_QC.flag_meanings:          no_qc_performed good_data probably_good_d...\n",
      "    CNDC_QC.long_name:              Electrical conductivity quality flag,\n",
      "               'data_attr': array(['58HJ', 'Helmer Hanssen', '2003', 'OceanSITES vertical profile',\n",
      "       'Arctic Ocean - In Situ Observation Copernicus', 'CTD Seabird',\n",
      "       '1.000', '792.000', '6.303', '39.826', '63.889', '81.179'],\n",
      "      dtype='<U45')},\n",
      "        'JH': {'data': <xarray.Dataset>\n",
      "Dimensions:    (TIME: 950, LATITUDE: 950, LONGITUDE: 950, DEPTH: 26)\n",
      "Coordinates:\n",
      "  * TIME       (TIME) float64 1.938e+04 1.938e+04 ... 1.971e+04 1.971e+04\n",
      "  * LATITUDE   (LATITUDE) float32 70.4 70.5 70.75 71.01 ... 71.75 71.74 71.75\n",
      "  * LONGITUDE  (LONGITUDE) float32 31.32 31.29 31.23 31.22 ... 13.01 14.99 17.07\n",
      "Dimensions without coordinates: DEPTH\n",
      "Data variables:\n",
      "    TEMP       (TIME, DEPTH) float64 ...\n",
      "Attributes: (12/128)\n",
      "    _NCProperties:                  version=1|netcdflibversion=4.4.1|hdf5libv...\n",
      "    title:                          Arctic Ocean - In Situ Observation Copern...\n",
      "    qc_manual:                      Recommendations for in-situ data Near Rea...\n",
      "    contact:                        cmems-service@imr.no\n",
      "    format_version:                 1.4\n",
      "    distribution_statement:         These data follow Copernicus standards; t...\n",
      "    ...                             ...\n",
      "    CNDC_QC.conventions:            Copernicus Marine In Situ reference table 2\n",
      "    CNDC_QC.valid_min:              0\n",
      "    CNDC_QC.valid_max:              9\n",
      "    CNDC_QC.flag_values:            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "    CNDC_QC.flag_meanings:          no_qc_performed good_data probably_good_d...\n",
      "    CNDC_QC.long_name:              Electrical conductivity quality flag,\n",
      "               'data_attr': array(['58JH', 'Johan Hjort', '2003', 'OceanSITES vertical profile',\n",
      "       'Arctic Ocean - In Situ Observation Copernicus', 'CTD Seabird',\n",
      "       '1.000', '3766.000', '-15.502', '47.017', '50.998', '77.829'],\n",
      "      dtype='<U45')}},\n",
      " 2004: {'AA': {'data': <xarray.Dataset>\n",
      "Dimensions:    (TIME: 1206, LATITUDE: 1206, LONGITUDE: 1206, DEPTH: 26)\n",
      "Coordinates:\n",
      "  * TIME       (TIME) float64 1.975e+04 1.975e+04 ... 2.007e+04 2.007e+04\n",
      "  * LATITUDE   (LATITUDE) float32 60.75 60.75 60.75 60.76 ... 66.15 65.94 65.86\n",
      "  * LONGITUDE  (LONGITUDE) float32 4.611 4.451 4.295 4.117 ... 12.95 12.98 13.17\n",
      "Dimensions without coordinates: DEPTH\n",
      "Data variables:\n",
      "    TEMP       (TIME, DEPTH) float64 ...\n",
      "Attributes: (12/128)\n",
      "    _NCProperties:                  version=1|netcdflibversion=4.4.1|hdf5libv...\n",
      "    title:                          Arctic Ocean - In Situ Observation Copern...\n",
      "    qc_manual:                      Recommendations for in-situ data Near Rea...\n",
      "    contact:                        cmems-service@imr.no\n",
      "    format_version:                 1.4\n",
      "    distribution_statement:         These data follow Copernicus standards; t...\n",
      "    ...                             ...\n",
      "    CNDC_QC.conventions:            Copernicus Marine In Situ reference table 2\n",
      "    CNDC_QC.valid_min:              0\n",
      "    CNDC_QC.valid_max:              9\n",
      "    CNDC_QC.flag_values:            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "    CNDC_QC.flag_meanings:          no_qc_performed good_data probably_good_d...\n",
      "    CNDC_QC.long_name:              Electrical conductivity quality flag,\n",
      "               'data_attr': array(['58AA', 'H\\\\303\\\\245kon Mosby', '2004',\n",
      "       'OceanSITES vertical profile',\n",
      "       'Arctic Ocean - In Situ Observation Copernicus',\n",
      "       'CTD Seabird Sonder uspes.; CTD Seabird', '1.000', '2183.000',\n",
      "       '-23.754', '35.047', '56.643', '81.306'], dtype='<U45')},\n",
      "        'GS': {'data': <xarray.Dataset>\n",
      "Dimensions:    (TIME: 583, LATITUDE: 583, LONGITUDE: 583, DEPTH: 26)\n",
      "Coordinates:\n",
      "  * TIME       (TIME) float64 1.974e+04 1.974e+04 ... 2.007e+04 2.008e+04\n",
      "  * LATITUDE   (LATITUDE) float32 61.11 61.1 61.06 61.12 ... 68.39 68.43 68.37\n",
      "  * LONGITUDE  (LONGITUDE) float32 5.095 5.177 5.347 5.64 ... 16.09 16.7 16.05\n",
      "Dimensions without coordinates: DEPTH\n",
      "Data variables:\n",
      "    TEMP       (TIME, DEPTH) float64 ...\n",
      "Attributes: (12/152)\n",
      "    _NCProperties:                  version=1|netcdflibversion=4.4.1|hdf5libv...\n",
      "    title:                          Arctic Ocean - In Situ Observation Copern...\n",
      "    qc_manual:                      Recommendations for in-situ data Near Rea...\n",
      "    contact:                        cmems-service@imr.no\n",
      "    format_version:                 1.4\n",
      "    distribution_statement:         These data follow Copernicus standards; t...\n",
      "    ...                             ...\n",
      "    CNDC_DM._FillValue:              \n",
      "    CNDC_DM.conventions:            Copernicus Marine In Situ reference table 1\n",
      "    CNDC_DM.flag_values:            R, A, D\n",
      "    CNDC_DM.flag_meanings:          real-time adjusted-in-real-time delayed-mode\n",
      "    CNDC_DM.long_name:              Electrical conductivity method of data pr...\n",
      "    CNDC_DM.string_length:          4381,\n",
      "               'data_attr': array(['58GS', 'G.O. Sars', '2004', 'OceanSITES vertical profile',\n",
      "       'Arctic Ocean - In Situ Observation Copernicus', 'CTD Seabird',\n",
      "       '1.000', '4383.000', '-36.811', '36.426', '41.462', '78.44'],\n",
      "      dtype='<U45')},\n",
      "        'HJ': {'data': <xarray.Dataset>\n",
      "Dimensions:    (TIME: 237, LATITUDE: 237, LONGITUDE: 237, DEPTH: 26)\n",
      "Coordinates:\n",
      "  * TIME       (TIME) float64 1.983e+04 1.983e+04 ... 2.004e+04 2.004e+04\n",
      "  * LATITUDE   (LATITUDE) float32 71.82 73.55 76.78 75.48 ... 63.56 63.52 63.71\n",
      "  * LONGITUDE  (LONGITUDE) float32 29.11 32.79 31.23 30.48 ... 10.79 10.58 10.9\n",
      "Dimensions without coordinates: DEPTH\n",
      "Data variables:\n",
      "    TEMP       (TIME, DEPTH) float64 ...\n",
      "Attributes: (12/128)\n",
      "    _NCProperties:                  version=1|netcdflibversion=4.4.1|hdf5libv...\n",
      "    title:                          Arctic Ocean - In Situ Observation Copern...\n",
      "    qc_manual:                      Recommendations for in-situ data Near Rea...\n",
      "    contact:                        cmems-service@imr.no\n",
      "    format_version:                 1.4\n",
      "    distribution_statement:         These data follow Copernicus standards; t...\n",
      "    ...                             ...\n",
      "    CNDC_QC.conventions:            Copernicus Marine In Situ reference table 2\n",
      "    CNDC_QC.valid_min:              0\n",
      "    CNDC_QC.valid_max:              9\n",
      "    CNDC_QC.flag_values:            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "    CNDC_QC.flag_meanings:          no_qc_performed good_data probably_good_d...\n",
      "    CNDC_QC.long_name:              Electrical conductivity quality flag,\n",
      "               'data_attr': array(['58HJ', 'Helmer Hanssen', '2004', 'OceanSITES vertical profile',\n",
      "       'Arctic Ocean - In Situ Observation Copernicus', 'CTD Seabird',\n",
      "       '1.000', '2654.000', '2.974', '36.26', '63.258', '81.874'],\n",
      "      dtype='<U45')},\n",
      "        'JH': {'data': <xarray.Dataset>\n",
      "Dimensions:    (TIME: 1145, LATITUDE: 1145, LONGITUDE: 1145, DEPTH: 26)\n",
      "Coordinates:\n",
      "  * TIME       (TIME) float64 1.974e+04 1.974e+04 ... 2.007e+04 2.007e+04\n",
      "  * LATITUDE   (LATITUDE) float32 62.37 62.48 62.6 62.73 ... 71.08 71.49 71.46\n",
      "  * LONGITUDE  (LONGITUDE) float32 5.196 4.945 4.689 4.422 ... 15.99 16.05 14.61\n",
      "Dimensions without coordinates: DEPTH\n",
      "Data variables:\n",
      "    TEMP       (TIME, DEPTH) float64 ...\n",
      "Attributes: (12/128)\n",
      "    _NCProperties:                  version=1|netcdflibversion=4.4.1|hdf5libv...\n",
      "    title:                          Arctic Ocean - In Situ Observation Copern...\n",
      "    qc_manual:                      Recommendations for in-situ data Near Rea...\n",
      "    contact:                        cmems-service@imr.no\n",
      "    format_version:                 1.4\n",
      "    distribution_statement:         These data follow Copernicus standards; t...\n",
      "    ...                             ...\n",
      "    CNDC_QC.conventions:            Copernicus Marine In Situ reference table 2\n",
      "    CNDC_QC.valid_min:              0\n",
      "    CNDC_QC.valid_max:              9\n",
      "    CNDC_QC.flag_values:            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "    CNDC_QC.flag_meanings:          no_qc_performed good_data probably_good_d...\n",
      "    CNDC_QC.long_name:              Electrical conductivity quality flag,\n",
      "               'data_attr': array(['58JH', 'Johan Hjort', '2004', 'OceanSITES vertical profile',\n",
      "       'Arctic Ocean - In Situ Observation Copernicus',\n",
      "       'CTD Seabird CTD Seabird; Sonder uspes.', '1.000', '3730.000',\n",
      "       '-16.993', '49.063', '52.244', '80.503'], dtype='<U45')}}}\n",
      "\n",
      "2.2) metadata_yr\n",
      "{2003: {'AA': {'depth_abs_v1': 0,\n",
      "               'depth_abs_v2': 2809,\n",
      "               'depth_m_v1': 0,\n",
      "               'depth_m_v2': 25,\n",
      "               'depth_newindex4xr_v1': 0,\n",
      "               'depth_newindex4xr_v2': 25,\n",
      "               'depth_newindex_v1': 0,\n",
      "               'depth_newindex_v2': 25,\n",
      "               'vmin': 0},\n",
      "        'GS': {'depth_abs_v1': 0,\n",
      "               'depth_abs_v2': 3683,\n",
      "               'depth_m_v1': 0,\n",
      "               'depth_m_v2': 25,\n",
      "               'depth_newindex4xr_v1': 0,\n",
      "               'depth_newindex4xr_v2': 25,\n",
      "               'depth_newindex_v1': 0,\n",
      "               'depth_newindex_v2': 25,\n",
      "               'vmin': 1},\n",
      "        'GT': {'depth_abs_v1': 0,\n",
      "               'depth_abs_v2': 2956,\n",
      "               'depth_m_v1': 0,\n",
      "               'depth_m_v2': 25,\n",
      "               'depth_newindex4xr_v1': 0,\n",
      "               'depth_newindex4xr_v2': 25,\n",
      "               'depth_newindex_v1': 0,\n",
      "               'depth_newindex_v2': 25,\n",
      "               'vmin': 1},\n",
      "        'HJ': {'depth_abs_v1': 0,\n",
      "               'depth_abs_v2': 789,\n",
      "               'depth_m_v1': 0,\n",
      "               'depth_m_v2': 25,\n",
      "               'depth_newindex4xr_v1': 0,\n",
      "               'depth_newindex4xr_v2': 25,\n",
      "               'depth_newindex_v1': 0,\n",
      "               'depth_newindex_v2': 25,\n",
      "               'vmin': 1},\n",
      "        'JH': {'depth_abs_v1': 0,\n",
      "               'depth_abs_v2': 3763,\n",
      "               'depth_m_v1': 0,\n",
      "               'depth_m_v2': 25,\n",
      "               'depth_newindex4xr_v1': 0,\n",
      "               'depth_newindex4xr_v2': 25,\n",
      "               'depth_newindex_v1': 0,\n",
      "               'depth_newindex_v2': 25,\n",
      "               'vmin': 1}},\n",
      " 2004: {'AA': {'depth_abs_v1': 0,\n",
      "               'depth_abs_v2': 2181,\n",
      "               'depth_m_v1': 0,\n",
      "               'depth_m_v2': 25,\n",
      "               'depth_newindex4xr_v1': 0,\n",
      "               'depth_newindex4xr_v2': 25,\n",
      "               'depth_newindex_v1': 0,\n",
      "               'depth_newindex_v2': 25,\n",
      "               'vmin': 1},\n",
      "        'GS': {'depth_abs_v1': 0,\n",
      "               'depth_abs_v2': 4380,\n",
      "               'depth_m_v1': 0,\n",
      "               'depth_m_v2': 25,\n",
      "               'depth_newindex4xr_v1': 0,\n",
      "               'depth_newindex4xr_v2': 25,\n",
      "               'depth_newindex_v1': 0,\n",
      "               'depth_newindex_v2': 25,\n",
      "               'vmin': 1},\n",
      "        'HJ': {'depth_abs_v1': 0,\n",
      "               'depth_abs_v2': 2650,\n",
      "               'depth_m_v1': 0,\n",
      "               'depth_m_v2': 25,\n",
      "               'depth_newindex4xr_v1': 0,\n",
      "               'depth_newindex4xr_v2': 25,\n",
      "               'depth_newindex_v1': 0,\n",
      "               'depth_newindex_v2': 25,\n",
      "               'vmin': 1},\n",
      "        'JH': {'depth_abs_v1': 0,\n",
      "               'depth_abs_v2': 3726,\n",
      "               'depth_m_v1': 0,\n",
      "               'depth_m_v2': 25,\n",
      "               'depth_newindex4xr_v1': 0,\n",
      "               'depth_newindex4xr_v2': 25,\n",
      "               'depth_newindex_v1': 0,\n",
      "               'depth_newindex_v2': 25,\n",
      "               'vmin': 1}}}\n",
      "\n",
      "2.3) vmin_dict_yr\n",
      "{2003: {'GS': {'TEMP': False},\n",
      "        'GT': {'TEMP': False},\n",
      "        'HJ': {'TEMP': False},\n",
      "        'JH': {'TEMP': False}},\n",
      " 2004: {'AA': {'TEMP': False},\n",
      "        'GS': {'TEMP': False},\n",
      "        'HJ': {'TEMP': False},\n",
      "        'JH': {'TEMP': False}}}\n"
     ]
    }
   ],
   "source": [
    "global data_dict_yr, metadata_yr, vmin_dict_yr, filtered_xarr_dict\n",
    "data_dict_yr = {}\n",
    "metadata_yr = {}\n",
    "vmin_dict_yr = {}\n",
    "\n",
    "for year in range(year1, year2+1): # need to do a for loop over the years as the data is saved in years on the server\n",
    "    \n",
    "    # Extract all platform_codes for that year\n",
    "    pc_sel = [pc for pc in dds_year_dict[year].keys()]\n",
    "    print(f'Working on year: {year} - Available platforms: {pc_sel}')\n",
    "\n",
    "    data_dict_yr[year], metadata_yr[year] = extractVARsAndDepth(pc_sel, pos_year_dict[year], dds_year_dict[year], url_info, year) \n",
    "\n",
    "    print(f'Attributes Year: {year}')\n",
    "    # Create overview dataframe\n",
    "    overview_df = pd.DataFrame()\n",
    "    overview_df = getAttributes(overview_df, data_dict_yr[year])\n",
    "    print('overview_df')\n",
    "    display(overview_df)\n",
    "\n",
    "    # Generate vmin dictionary (needed to avoid doing the vmin adjustment more than once)\n",
    "    vmin_dict_yr[year] = getVminDict(overview_df)\n",
    "    \n",
    "print('\\n2) ===================Printing Results:')\n",
    "print('\\n2.1) data_dict_yr')\n",
    "pprint.pprint(data_dict_yr)\n",
    "print('\\n2.2) metadata_yr')\n",
    "pprint.pprint(metadata_yr)\n",
    "print('\\n2.3) vmin_dict_yr')\n",
    "pprint.pprint(vmin_dict_yr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Depth and Indices (generated by BBOX and Time indices)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Longitude_WGS84</th>\n",
       "      <th>Latitude_WGS84</th>\n",
       "      <th>Time</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Index_ABS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>5.098700</td>\n",
       "      <td>61.916801</td>\n",
       "      <td>2003-12-01 06:06:42</td>\n",
       "      <td>AA</td>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>5.350700</td>\n",
       "      <td>62.132198</td>\n",
       "      <td>2003-12-01 15:04:22</td>\n",
       "      <td>AA</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>5.458200</td>\n",
       "      <td>62.091702</td>\n",
       "      <td>2003-12-01 16:25:02</td>\n",
       "      <td>AA</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>5.511800</td>\n",
       "      <td>62.060001</td>\n",
       "      <td>2003-12-01 17:00:58</td>\n",
       "      <td>AA</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>5.500700</td>\n",
       "      <td>62.035301</td>\n",
       "      <td>2003-12-01 17:28:26</td>\n",
       "      <td>AA</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5294</th>\n",
       "      <td>31.205500</td>\n",
       "      <td>71.252296</td>\n",
       "      <td>2004-01-27 00:42:36</td>\n",
       "      <td>JH</td>\n",
       "      <td>2085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5295</th>\n",
       "      <td>31.212500</td>\n",
       "      <td>71.005798</td>\n",
       "      <td>2004-01-27 02:46:14</td>\n",
       "      <td>JH</td>\n",
       "      <td>2086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5296</th>\n",
       "      <td>31.215300</td>\n",
       "      <td>70.751701</td>\n",
       "      <td>2004-01-27 04:33:22</td>\n",
       "      <td>JH</td>\n",
       "      <td>2087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5297</th>\n",
       "      <td>31.217501</td>\n",
       "      <td>70.498703</td>\n",
       "      <td>2004-01-27 06:34:35</td>\n",
       "      <td>JH</td>\n",
       "      <td>2088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5298</th>\n",
       "      <td>31.217199</td>\n",
       "      <td>70.397301</td>\n",
       "      <td>2004-01-27 07:22:52</td>\n",
       "      <td>JH</td>\n",
       "      <td>2089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Longitude_WGS84  Latitude_WGS84                Time Platform  Index_ABS\n",
       "647          5.098700       61.916801 2003-12-01 06:06:42       AA        647\n",
       "648          5.350700       62.132198 2003-12-01 15:04:22       AA        648\n",
       "649          5.458200       62.091702 2003-12-01 16:25:02       AA        649\n",
       "650          5.511800       62.060001 2003-12-01 17:00:58       AA        650\n",
       "651          5.500700       62.035301 2003-12-01 17:28:26       AA        651\n",
       "...               ...             ...                 ...      ...        ...\n",
       "5294        31.205500       71.252296 2004-01-27 00:42:36       JH       2085\n",
       "5295        31.212500       71.005798 2004-01-27 02:46:14       JH       2086\n",
       "5296        31.215300       70.751701 2004-01-27 04:33:22       JH       2087\n",
       "5297        31.217501       70.498703 2004-01-27 06:34:35       JH       2088\n",
       "5298        31.217199       70.397301 2004-01-27 07:22:52       JH       2089\n",
       "\n",
       "[162 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need to understand why hte index is not working properly... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by Depth and Indices (generated by BBOX and Time indices) \n",
    "filtered_xarr_dict = {}\n",
    "filtered_xarr_dict = filterbyDepthAndIndices1(data_dict_yr, metadata_yr, vmin_dict_yr, df_filtered)\n",
    "print('\\n4) filtered_xarr_dict', filtered_xarr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterbyDepthAndIndices1(data_dict_yr, metadata_yr, vmin_dict_yr, df_filtered):\n",
    "    \n",
    "    print(f'\\n3) Selected DEPTH: {depth_g}m')\n",
    "    \n",
    "    for year in data_dict_yr.keys():\n",
    "        print(year)\n",
    "        filtered_xarr_dict[year] = {}\n",
    "        \n",
    "        for pc in data_dict_yr[year].keys():\n",
    "            print(pc)\n",
    "            # Generate a filtered xarray with all variables for selected Platform, for a certain DEPTH range\n",
    "            \n",
    "            if metadata_yr[year][pc]['depth_m_v1']==0: align_and_nan = True\n",
    "            else: align_and_nan = False\n",
    "\n",
    "            for v in vars_g: \n",
    "                check_alignment1(data_dict_yr[year], pc, v, align_and_nan, vmin_dict_yr[year]) \n",
    "\n",
    "            filtered_xarr_dict[year][pc] = filter_xarr_DEPTH1(df_filtered, \n",
    "                                                             data_dict_yr[year],\n",
    "                                                             platform=pc,\n",
    "                                                             depth_range=[depth_g, depth_g])\n",
    "            print(filtered_xarr_dict[year][pc])\n",
    "        print('\\n')\n",
    "    \n",
    "    return filtered_xarr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check whether data should be aligned if vmin = 1, and align if so if has not been done already\n",
    "def check_alignment1(data_dict, pc, var, align_and_nan, vmin_dict):\n",
    "    \n",
    "    xarr = data_dict[pc]['data']\n",
    "    xarr_var = xarr[var].data\n",
    "    \n",
    "    vmin = float(xarr.attrs['geospatial_vertical_min'])\n",
    "\n",
    "    if vmin == 0:\n",
    "        print(f'Platform: {pc}; Vertical min: {vmin}; Var: {var}')\n",
    "        \n",
    "    elif vmin==1 and vmin_dict[pc][var]==False and align_and_nan: \n",
    "        # shift to the right and add nan in first position \n",
    "        print(f'Platform: {pc}; Vertical min: {vmin}; Var: {var} --> aligning and add nan')        \n",
    "        data_dict[pc]['data'][var].data = adjust_with_vmin(xarr_var, value=np.nan)\n",
    "        vmin_dict[pc][var] = True # to avoid doing hte vmin adjustment for this pc/var more than once  \n",
    "        \n",
    "    elif vmin==1 and vmin_dict[pc][var]==False and not align_and_nan: \n",
    "        # No need to shift, this occurred already in the data extraction\n",
    "        print(f'Platform: {pc}; Vertical min: {vmin}; Var: {var} --> data has been aligned already')\n",
    "        vmin_dict[pc][var] = True # to avoid doing hte vmin adjustment for this pc/var more than once\n",
    "    \n",
    "    \n",
    "#     return data_dict[pc], vmin_dict[pc][var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need to understand why hte index is not working properly... below is where the error is, between index and data_dict[platform]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter XARRAY based on platform, Var and DEPTH\n",
    "def filter_xarr_DEPTH1(df_toPlot, data_dict, platform, depth_range):\n",
    "    display(df_toPlot)\n",
    "#     pprint.pprint(data_dict)\n",
    "   \n",
    "    # find indices for each platform for the selected data\n",
    "    index = df_toPlot[df_toPlot['Platform']==platform].index.tolist()\n",
    "    print(index)\n",
    "    \n",
    "    display(data_dict[platform]['data'])\n",
    "    \n",
    "    # Filer data using the indexes of the filtered elements\n",
    "    xarr_sel = data_dict[platform]['data'].isel(TIME=index,\n",
    "                                                LATITUDE=index,\n",
    "                                                LONGITUDE=index,\n",
    "                                                DEPTH=slice(depth_range[0], depth_range[1]+1))\n",
    "    return xarr_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[df_filtered['Platform']=='AA'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter by BBOX, adding Time Range filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define here start and end date in the format [day,month,year]\n",
    "time_start = [1,12,2003] \n",
    "time_end = [31,1,2004] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = datetime(time_start[2], time_start[1], time_start[0])\n",
    "time_end = datetime(time_end[2], time_end[1], time_end[0])\n",
    "\n",
    "position_df_bbox_timerange = position_df_bbox.loc[(position_df_bbox['Time']>=time_start) & \n",
    "                                                  (position_df_bbox['Time']<=time_end)]\n",
    "\n",
    "# Print filtering results on original dataframe\n",
    "print('Year:', year)\n",
    "print('BBOX:', bbox_key)\n",
    "time_filter_str = f'{time_start.strftime(\"%Y%m%d\")}-{time_end.strftime(\"%Y%m%d\")}'\n",
    "print(f'Time Filter: {time_filter_str}')\n",
    "sel_outof_all = f'{len(position_df_bbox_timerange)} out of {len(position_df)}.'\n",
    "print(f'Selected positions (out of available positions): {sel_outof_all}')\n",
    "\n",
    "display(position_df_bbox_timerange)\n",
    "\n",
    "title = f'Filtered data: {bbox_key} and Time={time_filter_str}'\n",
    "plotInteractive(position_df_bbox_timerange, title, 'Longitude', 'Latitude', xlim, ylim, bbox_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print or Export Filtered Positions\n",
    "Uncomment the rows below if you want to display or export to CSV the filtered dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print filtered dataframe\n",
    "# pd.set_option('display.max_rows', None) # pd.set_option('display.max_rows', 10) # to restore first- and last- 5 rows to display\n",
    "# display(position_df_bbox_timerange)\n",
    "\n",
    "# # Save dataframe to csv\n",
    "# data_output = os.path.join(os.getcwd(), 'data_output')\n",
    "# if not os.path.exists(data_output): os.mkdir(data_output)\n",
    "# csvname = os.path.join(data_output, f'filtered_{pc}_df.csv')\n",
    "# position_df_bbox_timerange.to_csv(csvname, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zoom in on Filtered Positions\n",
    "Plot filtered positions with the relavite extent of those positions only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim_small = (min(position_df_bbox_timerange['Longitude'])-1, max(position_df_bbox_timerange['Longitude'])+1)\n",
    "ylim_small = (min(position_df_bbox_timerange['Latitude'])-1, max(position_df_bbox_timerange['Latitude'])+1)\n",
    "title = f'Filtered data: {bbox_key} and Time={time_filter_str}'\n",
    "plotInteractive(position_df_bbox_timerange, title, 'Longitude', 'Latitude', xlim_small, ylim_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Filtered Dataframe (*df_toPlot*) to be used for online querying of the filtered positions\n",
    "Define the filtered dataframe (eg *position_df_bbox*, *position_df_bbox_timerange*), to be named **df_toPlot**, and the dictionary of indices of filtered data (to be named **index_dict**), to use for further filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataframe to plot based on one of the previously defined filters\n",
    "df_toPlot = position_df_bbox_timerange # or position_df / position_df_bbox\n",
    "\n",
    "sel_outof_all = f'{len(df_toPlot)} / {len(position_df)}.'\n",
    "\n",
    "print(f'- Filters: \"BBOX={bbox_key}\" and \"Time={time_filter_str}\"\\n- Filtered / All (out of available positions): {sel_outof_all}')\n",
    "\n",
    "display(df_toPlot)\n",
    "\n",
    "# Dictionary of indices\n",
    "index_dict = {}\n",
    "\n",
    "for pc in df_toPlot['Platform'].unique():\n",
    "    index_dict[pc] = df_toPlot[df_toPlot['Platform']==pc].index.tolist()\n",
    "\n",
    "index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_toPlot[df_toPlot['Platform']=='JH'])\n",
    "df_toPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Up to here is comparable to the execution \"extractor-tool --depth 25 --bbox \"-20, 40, 50, 85\" --time1 20031201 --time2 20040131 --vars 'TEMP' --format 'csv'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "The CTD data is extracted based on the following parameters: Variables, Depth range, and Platform code. \n",
    "\n",
    "Afterwards, an additional filtering is applied based on the list of indices that will be extracted from the dictionary of positions (*df_toPlot*). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Variable(s)\n",
    "vars_main = ['PRES', 'TEMP', 'PSAL', 'CNDC'] \n",
    "\n",
    "# Define the selection of variable to use for the analysis\n",
    "vars_sel = ['TEMP']#, 'PRES'] #, 'CNDC', 'PSAL']; \n",
    "assert all([elem in vars_main for elem in vars_sel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define depths limits\n",
    "depth1 = 25\n",
    "depth2 = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Platform(s) - Select either one of the two options below  \n",
    "\n",
    "#================================================\n",
    "# Option A) Use data from ALL available platforms\n",
    "pc_sel = df_toPlot['Platform'].unique()\n",
    "\n",
    "# Option B) Use data from only ONE platform\n",
    "# pc_sel = ['AA']; assert pc_sel in df_toPlot['Platform'].unique(), 'ERROR: platform not available in given year.'\n",
    "#================================================\n",
    "\n",
    "# Create string for output name\n",
    "if len(pc_sel) == 1: pc_str = pc_sel[0]\n",
    "else: pc_str = \"-\".join(pc_sel)\n",
    "    \n",
    "# Print selection\n",
    "print(f'Platform(s) selected: {pc_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create **data_dict** dictionary \n",
    "Data and their attributes are read iteratively for selected platform(s), and saved into a dictionary *data_dict* which contains:\n",
    "* the actual data, loaded into an **xarray** for data handling, analysis and visualisation\n",
    "* the campaign's main attributes: platform code & name, data type, title, instrument, longitude & latitude, and vertical min & max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2003, 2004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dds_year_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    \n",
    "    # Extract all platform_codes for that year\n",
    "    pc_sel = [pc for pc in dds_year_dict[year].keys()]\n",
    "    print(f'Working on year: {year} - Available platforms: {pc_sel}')\n",
    "        \n",
    "    \n",
    "    \n",
    "    data_dict = {}\n",
    "    metadata = {}\n",
    "\n",
    "    for pc in pc_sel:\n",
    "\n",
    "        metadata[pc] = {}\n",
    "\n",
    "        v_min = int(float(position_dict[pc]['data_attr'][6]))\n",
    "        metadata[pc]['vmin'] = v_min\n",
    "        metadata[pc]['depth_abs_v1'] = 0 # this is fixed\n",
    "        metadata[pc]['depth_abs_v2'] = pc_dim_dict[pc]['DEPTH'] # this is fixed\n",
    "\n",
    "        # ==============================================================================\n",
    "        \"\"\"\n",
    "        Define here the DEPTH range of your selection, in meters. Note that either:\n",
    "        - 'depth_m_v1' is equal to the lower bound (ie index=0), or \n",
    "        - 'depth_m_v2' is equal to the upper bound (ie index=-1)\n",
    "        \"\"\"\n",
    "        metadata[pc]['depth_m_v1'] = 0 # depth1\n",
    "        metadata[pc]['depth_m_v2'] = depth2 # pc_dim_dict[pc]['DEPTH']\n",
    "        # ==============================================================================\n",
    "\n",
    "        # assert metadata[pc]['depth_m_v1'] < metadata[pc]['depth_m_v2'], 'ERROR: the lower bound must be lower than the higher bound' \n",
    "        # assert metadata[pc]['depth_m_v1'] == 0 or metadata[pc]['depth_m_v2'] == pc_dim_dict[pc]['DEPTH'], 'ERROR: one of the two values must be equal to one of the lower/upper bounds'\n",
    "\n",
    "        #     print(f'DEPTH range of interest (meters): {metadata[pc][\"depth_m_v1\"]} - {metadata[pc][\"depth_m_v2\"]}')\n",
    "\n",
    "        # the start and stop values are adjusted based on the vmin value\n",
    "        if metadata[pc]['vmin'] == 1: \n",
    "            if metadata[pc]['depth_m_v1'] == 0: # \n",
    "                metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1'] # the same\n",
    "                metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2'] # the same, so I have the right size. When I shift and add the nan, I get rid of further element on the right\n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2']# - 1\n",
    "\n",
    "            elif metadata[pc]['depth_m_v1'] != 0: \n",
    "                metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1'] - 1 # start one element before\n",
    "                metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2'] - 1 # last element is excluded, ie stop one element before. But then I'll have to remoove one element\n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2'] - metadata[pc]['depth_m_v1'] - 1 \n",
    "\n",
    "        else:\n",
    "            metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1']\n",
    "            metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2']\n",
    "\n",
    "            if metadata[pc]['depth_m_v1'] == 0: # \n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2']\n",
    "\n",
    "            elif metadata[pc]['depth_m_v1'] != 0: \n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2'] - metadata[pc]['depth_m_v1']\n",
    "\n",
    "        metadata[pc]['depth_newindex4xr_v1'] = 0\n",
    "\n",
    "        pprint.pprint(metadata[pc])\n",
    "        print(f'{pc} DEPTH range of interest (adjusted with vmin): {metadata[pc][\"depth_newindex_v1\"]} - {metadata[pc][\"depth_newindex_v2\"]}')\n",
    "\n",
    "        fix_lab = f'58{pc}_CTD_{year}' # platform_codes and year are defined at the beginning of the notebook \n",
    "\n",
    "        # Get coordinates (needed for keeping hte correct structure, and for plotting) \n",
    "        coords_str = getQueryString(pc_dim_dict[pc], keylist = ['TIME', 'LATITUDE', 'LONGITUDE']) # list the coordinates you want\n",
    "\n",
    "        # Extract TIME and DEPTH dimension for queries \n",
    "        time_dims = getQuery(pc, start=0, stop=pc_dim_dict[pc]['TIME'])\n",
    "        depth_dims = getQuery(pc, start=metadata[pc]['depth_newindex_v1'], stop=metadata[pc]['depth_newindex_v2'])#; print(depth_dims)\n",
    "\n",
    "        # join TIME and DEPTH for Variables\n",
    "        var_str_ALL = []\n",
    "        for v in vars_sel: var_str_ALL = np.append(var_str_ALL, f'{v}{time_dims}{depth_dims}')\n",
    "        queries_vars = ','.join(var_str_ALL)\n",
    "\n",
    "        # Build url and url with queries (url_q)\n",
    "        url = f'{url_info[0]}{fix_lab}.nc{url_info[1]}?{coords_str}' \n",
    "        url_q = f'{url},{queries_vars}'; print(f'Platform {pc} URL:', url_q)\n",
    "\n",
    "        remote_data, data_attr = fetch_data(url_q, year)\n",
    "\n",
    "        data_dict[pc] = {'data': remote_data, \n",
    "                         'data_attr': data_attr}\n",
    "\n",
    "        print(f'{data_attr}\\n')\n",
    "\n",
    "    # display(data_dict)\n",
    "    print(f'Checking the existing campaigns in the dictionary: {list(data_dict.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Overview Dataframe with Platforms' Attributes\n",
    "An overview dataframe *overview_df* is then generated to show the detailed information about each campaign at sea: platform code & name, data type, title, instrument, longitude & latitude, and vertical min & max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database with the selected campaigns & years\n",
    "overview_df = pd.DataFrame()\n",
    "overview_df = getAttributes(overview_df, data_dict)\n",
    "overview_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract / define the variables to use for the analysis\n",
    "print('\\nPrinting DEPTH range for analyis:')\n",
    "assert len(np.unique([metadata[k][\"depth_m_v1\"] for k in metadata.keys()])==1)\n",
    "assert len(np.unique([metadata[k][\"depth_m_v2\"] for k in metadata.keys()])==1)\n",
    "\n",
    "for k in data_dict.keys():\n",
    "    print(f'{k}; DEPTH filtered: {metadata[k][\"depth_m_v1\"]}-{metadata[k][\"depth_m_v2\"]}m; VARS: {list(data_dict[k][\"data\"].variables)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate vmin dictionary (needed to avoid doing the vmin adjustment more than once)\n",
    "vmin_dict = {}\n",
    "\n",
    "# select only those platforms where vmin == 1\n",
    "vmin_pc = overview_df[overview_df['Vertical_min'] == 1.0].index\n",
    "\n",
    "for i in vmin_pc:\n",
    "    vmin_dict[i] = {}\n",
    "    \n",
    "    for v in vars_sel:\n",
    "        vmin_dict[i][v] = False\n",
    "\n",
    "vmin_dict   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by DEPTH and selected Indices\n",
    "Apply the previously defined selection of indices (filtered based on BBOX and Time Range), and add filtering by DEPTH range. \n",
    "\n",
    "The output of this operation is a *filtered_xarr_dict* dictionary containing xarray datasets for each platform, containing the variable at the specified DEPTH range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data filtered by DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_xarr_dict = {}\n",
    "\n",
    "print(f'Selected range of DEPTH: {depth1} - {depth2}m\\n')\n",
    "for pc in data_dict.keys():\n",
    "\n",
    "    # Generate a filtered xarray with all variables for selected Platform, for a certain DEPTH range\n",
    "    if metadata[pc]['depth_m_v1']==0: align_and_nan = True\n",
    "    else: align_and_nan = False\n",
    "\n",
    "    for v in vars_sel: \n",
    "        check_alignment(data_dict, pc, v, align_and_nan, vmin_dict)\n",
    "\n",
    "    filtered_xarr_dict[pc] = filter_xarr_DEPTH(df_toPlot, \n",
    "                                               data_dict,\n",
    "                                               platform=pc,\n",
    "                                               depth_range=[depth1, depth2])\n",
    "    # display(filtered_xarr_dict[pc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation of Available Platforms\n",
    "Two xarray datasets can be merged if they have the same structure, i.e. dimensions. First check the dimensions of DEPTH are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of variables for each platform\n",
    "data_var_dict = {}\n",
    "depth_arr = []\n",
    "\n",
    "for pc in data_dict.keys():\n",
    "    \n",
    "    data_var_dict[pc] = {}\n",
    "    data = filtered_xarr_dict[pc]\n",
    "    \n",
    "    depth_dim_pc = data.dims[\"DEPTH\"]\n",
    "    depth_arr.append(depth_dim_pc)\n",
    "    \n",
    "    print(f'PC {pc}\\tFiltered Dims: TIME={data.dims[\"TIME\"]}, DEPTH={data.dims[\"DEPTH\"]}')\n",
    "    \n",
    "    for var in vars_sel:\n",
    "        data_var_dict[pc][var] = filtered_xarr_dict[pc][var]\n",
    "        \n",
    "assert all(x==depth_arr[0] for x in depth_arr), 'ERROR, the DEPTH dimensions must be equal.'\n",
    "# display(data_var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now aggregate and plot each variable: on the y-axis is shown the TIME of the measurement (in float format, which needs to be converted to datetime format), and on the x-axis is the DEPTH of the measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine arrays across platforms, for each variable\n",
    "merged_arr = {}\n",
    "\n",
    "for var in vars_sel:\n",
    "    \n",
    "    merged_arr[var] = xr.merge([data_var_dict[pc][var] for pc in data_dict.keys()])  \n",
    "        \n",
    "    title = f'Var={var} (Merged Platforms)\\nFilter: Time Range={time_filter_str};\\nBBOX={bbox_key}; Depth Range {depth1}-{depth2}m;\\nSel/All={sel_outof_all}'\n",
    "\n",
    "    plotVar_MergedPlatforms(merged_arr[var], var, title=title)\n",
    "    # display(merged_arr[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine arrays across platforms, with ALL variables\n",
    "merged_arr_vars = xr.merge([data_var_dict[pc][var] for pc in data_dict.keys() for var in vars_sel]) \n",
    "merged_arr_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = merged_arr_vars.to_dataframe().reset_index()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m['DEPTH'] = depth1\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[['TIME','DEPTH','TEMP']].to_csv('/workspace/INTAROS/iaos-CTD-extract-from-opendap/exported_data/test_temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['AA']['data'].isel(TIME=np.arange(0,2),\n",
    "                             LATITUDE=np.arange(0,2),\n",
    "                             LONGITUDE=np.arange(0,2),\n",
    "                             DEPTH=np.arange(0,1)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['AA']['data'].isel(TIME=np.arange(0,2),\n",
    "                             LATITUDE=np.arange(0,2),\n",
    "                             LONGITUDE=np.arange(0,2),\n",
    "                             DEPTH=np.arange(0,1)\n",
    "                            ).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_var_dict[pc]['TEMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Recap:\n",
    "* ```data_dict[pc]['data']```: contains the xarray extracted from the url (eg from **0** to **depth2**, for ALL locations)\n",
    "* ```filtered_xarr```: contains the xarray filtered from ```data_dict[pc]['data']``` (ie from **depth1** to **depth2**, for filtered locations (BBOX and time_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output dir\n",
    "data_output = os.path.join(os.getcwd(), 'data_output')\n",
    "if not os.path.exists(data_output): os.mkdir(data_output)\n",
    "\n",
    "# File name (without file extension)\n",
    "fname = os.path.join(data_output, \n",
    "                     f'Scientist_pc={pc_str}_BBOX={bbox_key}_MMYYYY={time_filter_str}_d={depth1}-{depth2}m_var={\"_\".join(vars_sel)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to NetCDF\n",
    "netcdfname = fname + '.nc.nc4'\n",
    "merged_arr['TEMP'].to_netcdf(path=netcdfname,\n",
    "                             mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to CSV\n",
    "##### Create Dataframe of Filtered XARRAY\n",
    "This step is implemented to generate a CSV-structured dataframe, to then export to a CSV file, which is the input expected by the RGeostats module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with filtered data and columns ['LATITUDE', 'LONGITUDE', 'TIME', 'TEMP', 'DEPTH']\n",
    "filtered2csv_multiDepths = pd.DataFrame() \n",
    "\n",
    "for pc in data_dict.keys():\n",
    "\n",
    "    for d in range(depth1, depth2+1):\n",
    "\n",
    "        # Create temporary dataframe\n",
    "        temp = pd.DataFrame()\n",
    "\n",
    "        data_depth_sel = data_dict[pc]['data'].isel(TIME=index_dict[pc],\n",
    "                                                    LATITUDE=index_dict[pc],\n",
    "                                                    LONGITUDE=index_dict[pc],\n",
    "                                                    DEPTH=slice(d, d+1))\n",
    "\n",
    "        for col in ['LONGITUDE', 'LATITUDE', 'TIME']:\n",
    "            temp[col.title()] = data_depth_sel[col].data.astype(float) \n",
    "\n",
    "        if 'TEMP' in vars_sel: temp['Temperature'] = data_depth_sel['TEMP'].data.astype(float) \n",
    "        if 'CNDC' in vars_sel: temp['Conductivity'] = data_depth_sel['CNDC'].data.astype(float) \n",
    "        if 'PSAL' in vars_sel: temp['Salinity'] = data_depth_sel['PSAL'].data.astype(float) \n",
    "\n",
    "        temp['Depth'] = d \n",
    "        temp['Vessel_name'] = pc \n",
    "\n",
    "        filtered2csv_multiDepths = filtered2csv_multiDepths.append(temp, ignore_index=True)\n",
    "    \n",
    "# Rename index column with 'rank'\n",
    "filtered2csv_multiDepths = filtered2csv_multiDepths.rename_axis(\"rank\")\n",
    "\n",
    "display(filtered2csv_multiDepths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assign *Profil_id* to the unique positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find pair of unique coordinates \n",
    "unique_pos = filtered2csv_multiDepths.groupby(['Longitude','Latitude']).size().reset_index().rename(columns={0:'count'})\n",
    "\n",
    "prof_id = 1\n",
    "for long, lat in zip(unique_pos['Longitude'], unique_pos['Latitude']):\n",
    "    \n",
    "    # Define condition\n",
    "    cond = (filtered2csv_multiDepths['Longitude'] == long) & (filtered2csv_multiDepths['Latitude'] == lat)\n",
    "#     display(filtered2csv_multiDepths.loc[cond])\n",
    "    \n",
    "    # Assign unique Profil_id \n",
    "    filtered2csv_multiDepths.loc[cond,'Profil_id'] = prof_id\n",
    "    prof_id += 1\n",
    "\n",
    "# Convert to integer\n",
    "filtered2csv_multiDepths = filtered2csv_multiDepths.astype({'Profil_id': int})\n",
    "display(filtered2csv_multiDepths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe to CSV\n",
    "csvname = fname + '.csv'\n",
    "filtered2csv_multiDepths.to_csv(csvname, sep=',', header=True)\n",
    "print('Output filename:', csvname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_intaros]",
   "language": "python",
   "name": "conda-env-env_intaros-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

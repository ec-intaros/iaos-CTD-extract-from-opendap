{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractor.py Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import xarray as xr\n",
    "import os\n",
    "from pyproj import Transformer\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "start_date = datetime(1950, 1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDates(time1,time2):\n",
    "    time_start = datetime(int(time1[:4]), int(time1[4:6]), int(time1[6:]))\n",
    "    time_end = datetime(int(time2[:4]), int(time2[4:6]), int(time2[6:]))\n",
    "    assert time_start < time_end, 'ERROR: time1 cannot be after time2, please check.'\n",
    "    \n",
    "    year1 = int(time1[:4])\n",
    "    year2 = int(time2[:4])\n",
    "    time_str = f'{time1}-{time2}'\n",
    "    \n",
    "    if year1==year2: sameyear = True\n",
    "    else: sameyear = False\n",
    "    \n",
    "    return time_start, time_end, year1, year2, time_str, sameyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkParams(depth, bbox, time1, time2, vars_sel, group, formats):\n",
    "    # Define Global Variables\n",
    "    global time_start, time_end, time_str, year1, year2, sameyear, bbox_g, bbox_str, depth_g, vars_g, group_g, formats_g\n",
    "     \n",
    "    # Check dates\n",
    "    time_start, time_end, year1, year2, time_str, sameyear = checkDates(time1,time2)\n",
    "    print('Time Range:', time_str)\n",
    "    print('Same Year flag:', sameyear)\n",
    "\n",
    "    # Check BBOX\n",
    "    bbox_g = [int(bb) for bb in bbox.split(',')]\n",
    "    bbox_str = '.'.join([str(b) for b in bbox_g])\n",
    "    print('Bounding Box:', bbox_g) #, bbox_str)\n",
    "\n",
    "    # Check depth\n",
    "    depth_g = depth\n",
    "    print(f'Depth: {depth_g}m')\n",
    "\n",
    "    # Check Vars\n",
    "    vars_g = vars_sel.split(',')\n",
    "    print('Vars:', vars_g)\n",
    "\n",
    "    # Check 'group' flag\n",
    "    if len(vars_g) > 1:\n",
    "        # the flag 'group' is required in this case\n",
    "        group_g = group\n",
    "        assert group_g is not None, 'The flag \"--group\" is required when multiple variables are selected.'\n",
    "        print('Group files per Var:', group_g)\n",
    "\n",
    "    else: group = False\n",
    "    \n",
    "    # Check Formats\n",
    "    formats = [ff.lower() for ff in formats.split(',')]    \n",
    "    formats_g = []\n",
    "    if 'csv' in formats: formats_g.append('CSV')\n",
    "    if 'netcdf4' in formats: formats_g.append('NetCDF4')\n",
    "    print('Output files format(s):', formats_g)\n",
    "    assert len(formats_g) > 0, 'ERROR: Output file format entered is wrong. It must be \"csv\", \"netcdf4\", or \"csv,netcdf4\".'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDDS(url_info, year):\n",
    "    # Get dds info, and assign max dimensions to TIME and DEPTH\n",
    "    \n",
    "    pc_dim_dict = {}\n",
    "    time_stop_dict = {}\n",
    "    depth_stop_dict = {}\n",
    "\n",
    "    # get all content of the server_url, and then filter it with year and available platforms\n",
    "    page = requests.get(url_info[0])\n",
    "    webpage = html.fromstring(page.content)\n",
    "    \n",
    "    urls_filtered = [p for p in webpage.xpath('//a/@href') if p.endswith(f'{year}.nc{url_info[1]}.dds')]\n",
    "\n",
    "    for u in urls_filtered:\n",
    "\n",
    "        dds = f'{url_info[0]}/{u}'#; print(dds)\n",
    "\n",
    "        # Find platform code\n",
    "        if len(url_info[1]) == 0: pc = dds.split('_')[0][-2:] # nmdc case\n",
    "        else: pc = dds.split('_')[1][-2:] # t2_hyrax case\n",
    "\n",
    "        pc_dim_dict[pc] = retrieveDDSinfo(dds)\n",
    "\n",
    "        time_stop_dict[pc] = pc_dim_dict[pc]['TIME']\n",
    "        depth_stop_dict[pc] = pc_dim_dict[pc]['DEPTH']\n",
    "\n",
    "    assert depth_stop_dict.keys() == time_stop_dict.keys(), 'TIME and DEPTH Keys error. Please check.'\n",
    "    \n",
    "    return pc_dim_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPositionDict(pc_dim_dict, url_info, year):\n",
    "    # Extract data and create position_dict\n",
    "    \n",
    "    position_dict = {}\n",
    "    \n",
    "    for pc in pc_dim_dict.keys():\n",
    "\n",
    "        coords_str = getQueryString(pc_dim_dict[pc], keylist = ['TIME', 'LATITUDE', 'LONGITUDE']) \n",
    "\n",
    "        fix_lab = f'58{pc}_CTD_{year}' # label to use for this campaign\n",
    "\n",
    "        url = f'{url_info[0]}{fix_lab}.nc{url_info[1]}?{coords_str}'; print(f'Platform: {pc}. URL with Queries:', url)\n",
    "\n",
    "        remote_data, data_attr = fetch_data(url, year)\n",
    "\n",
    "        position_dict[pc] = {'data': remote_data, \n",
    "                             'data_attr': data_attr}\n",
    "#     print(position_dict)    \n",
    "    \n",
    "    return position_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterBBOXandTIME(position_df, time1, time2):\n",
    "    # Filter the position_df dataframe by BBOX\n",
    "    position_df_bbox = position_df[(position_df.loc[:,'Longitude_WGS84'] >= bbox_g[0]) & \n",
    "                                   (position_df.loc[:,'Longitude_WGS84'] <= bbox_g[1]) & \n",
    "                                   (position_df.loc[:,'Latitude_WGS84'] >= bbox_g[2]) & \n",
    "                                   (position_df.loc[:,'Latitude_WGS84'] <= bbox_g[3])]\n",
    "\n",
    "    # Print filtering results on original dataframe\n",
    "    global sel_outof_all\n",
    "    sel_outof_all = f'{len(position_df_bbox)} out of {len(position_df)}.'\n",
    "#     print(f'Selected positions (out of available positions): {sel_outof_all}')\n",
    "#     print(position_df_bbox)\n",
    "\n",
    "    # Filter the position_df_bbox dataframe by TIME\n",
    "    position_df_bbox_timerange = position_df_bbox.loc[(position_df_bbox['Time']>=time_start) & \n",
    "                                                      (position_df_bbox['Time']<=time_end)]\n",
    "\n",
    "    # Print filtering results on original dataframe\n",
    "    print(f'\\nUser-defined Time Range: {time_str}')\n",
    "    sel_outof_all = f'{len(position_df_bbox_timerange)} out of {len(position_df)}.'\n",
    "    print(f'Selected positions (out of available positions): {sel_outof_all}')\n",
    "\n",
    "    # print(position_df_bbox_timerange)\n",
    "    \n",
    "    return position_df_bbox_timerange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndices(df_filtered):\n",
    "    index_dict = {}\n",
    "    \n",
    "    for pc in df_filtered['Platform'].unique():\n",
    "        index_dict[pc] = df_filtered[df_filtered['Platform']==pc].index.tolist()\n",
    "    \n",
    "    return index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractVARsAndDepth(pc_sel, position_dict, pc_dim_dict, url_info, year):  \n",
    "    data_dict = {}\n",
    "    metadata = {}\n",
    "\n",
    "    for pc in pc_sel:\n",
    "        print(pc)\n",
    "        metadata[pc] = {}\n",
    "\n",
    "        v_min = int(float(position_dict[pc]['data_attr'][6]))\n",
    "        metadata[pc]['vmin'] = v_min\n",
    "        metadata[pc]['depth_abs_v1'] = 0 # this is fixed\n",
    "        metadata[pc]['depth_abs_v2'] = pc_dim_dict[pc]['DEPTH'] # this is fixed\n",
    "\n",
    "        # ==============================================================================\n",
    "        \"\"\"\n",
    "        Define here the DEPTH range of your selection, in meters. Note that either:\n",
    "        - 'depth_m_v1' is equal to the lower bound (ie index=0), or \n",
    "        - 'depth_m_v2' is equal to the upper bound (ie index=-1)\n",
    "        \"\"\"\n",
    "        metadata[pc]['depth_m_v1'] = 0 \n",
    "        metadata[pc]['depth_m_v2'] = depth_g\n",
    "        # ==============================================================================\n",
    "\n",
    "        # assert metadata[pc]['depth_m_v1'] < metadata[pc]['depth_m_v2'], 'ERROR: the lower bound must be lower than the higher bound' \n",
    "        # assert metadata[pc]['depth_m_v1'] == 0 or metadata[pc]['depth_m_v2'] == pc_dim_dict[pc]['DEPTH'], 'ERROR: one of the two values must be equal to one of the lower/upper bounds'\n",
    "\n",
    "        #     print(f'DEPTH range of interest (meters): {metadata[pc][\"depth_m_v1\"]} - {metadata[pc][\"depth_m_v2\"]}')\n",
    "\n",
    "        # the start and stop values are adjusted based on the vmin value\n",
    "        if metadata[pc]['vmin'] == 1: \n",
    "            if metadata[pc]['depth_m_v1'] == 0: # \n",
    "                metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1'] # the same\n",
    "                metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2'] # the same, so I have the right size. When I shift and add the nan, I get rid of further element on the right\n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2']# - 1\n",
    "\n",
    "            elif metadata[pc]['depth_m_v1'] != 0: \n",
    "                metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1'] - 1 # start one element before\n",
    "                metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2'] - 1 # last element is excluded, ie stop one element before. But then I'll have to remoove one element\n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2'] - metadata[pc]['depth_m_v1'] - 1 \n",
    "\n",
    "        else:\n",
    "            metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1']\n",
    "            metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2']\n",
    "\n",
    "            if metadata[pc]['depth_m_v1'] == 0: # \n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2']\n",
    "\n",
    "            elif metadata[pc]['depth_m_v1'] != 0: \n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2'] - metadata[pc]['depth_m_v1']\n",
    "\n",
    "        metadata[pc]['depth_newindex4xr_v1'] = 0\n",
    "\n",
    "        pprint.pprint(metadata[pc])\n",
    "        print(f'{pc} DEPTH range of interest (adjusted with vmin): {metadata[pc][\"depth_newindex_v1\"]} - {metadata[pc][\"depth_newindex_v2\"]}')\n",
    "\n",
    "        fix_lab = f'58{pc}_CTD_{year}' # platform_codes and year are defined at the beginning of the notebook \n",
    "\n",
    "        # Get coordinates (needed for keeping the correct structure, and for plotting) \n",
    "        coords_str = getQueryString(pc_dim_dict[pc], keylist = ['TIME', 'LATITUDE', 'LONGITUDE']) # list the coordinates you want\n",
    "\n",
    "        # Extract TIME and DEPTH dimension for queries \n",
    "        time_dims = getQuery(pc, start=0, stop=pc_dim_dict[pc]['TIME'])\n",
    "        depth_dims = getQuery(pc, start=metadata[pc]['depth_newindex_v1'], stop=metadata[pc]['depth_newindex_v2'])#; print(depth_dims)\n",
    "\n",
    "        # join TIME and DEPTH for Variables\n",
    "        var_str_ALL = []\n",
    "        for v in vars_g: var_str_ALL = np.append(var_str_ALL, f'{v}{time_dims}{depth_dims}')\n",
    "        queries_vars = ','.join(var_str_ALL)\n",
    "\n",
    "        # Build url and url with queries (url_q)\n",
    "        url = f'{url_info[0]}{fix_lab}.nc{url_info[1]}?{coords_str}' \n",
    "        url_q = f'{url},{queries_vars}'; print(f'Platform {pc} URL:', url_q)\n",
    "\n",
    "        remote_data, data_attr = fetch_data(url_q, year)\n",
    "\n",
    "        data_dict[pc] = {'data': remote_data, \n",
    "                         'data_attr': data_attr}\n",
    "\n",
    "        print(f'{data_attr}\\n')\n",
    "\n",
    "    assert pc_sel == list(data_dict.keys()), 'ERROR: different platforms, please check.'\n",
    "    \n",
    "    return data_dict, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save Attributes to a database\n",
    "def getAttributes(my_df, my_dict):\n",
    "    \n",
    "    for key in my_dict.keys():\n",
    "\n",
    "        my_df.loc[key,'Platform_code'] = [my_dict[key]['data_attr'][0].astype(str)]\n",
    "        my_df.loc[key,'Platform_name'] = [my_dict[key]['data_attr'][1].astype(str)]\n",
    "        my_df.loc[key,'Year'] = [my_dict[key]['data_attr'][2].astype(int)]\n",
    "        my_df.loc[key,'Data_type'] = [my_dict[key]['data_attr'][3].astype(str)]\n",
    "        my_df.loc[key,'Title'] = [my_dict[key]['data_attr'][4].astype(str)]\n",
    "        my_df.loc[key,'Instrument'] = [my_dict[key]['data_attr'][5].astype(str)]\n",
    "        my_df.loc[key,'Vertical_min'] = [my_dict[key]['data_attr'][6].astype(float)]\n",
    "        my_df.loc[key,'Vertical_max'] = [my_dict[key]['data_attr'][7].astype(float)]\n",
    "        my_df.loc[key,'Lon_min'] = [my_dict[key]['data_attr'][8].astype(float)]\n",
    "        my_df.loc[key,'Lon_max'] = [my_dict[key]['data_attr'][9].astype(float)]\n",
    "        my_df.loc[key,'Lat_min'] = [my_dict[key]['data_attr'][10].astype(float)]\n",
    "        my_df.loc[key,'Lat_max'] = [my_dict[key]['data_attr'][11].astype(float)]\n",
    "\n",
    "    return my_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVminDict(overview_df):\n",
    "    vmin_dict = {}\n",
    "\n",
    "    # select only those platforms where vmin == 1\n",
    "    vmin_pc = overview_df[overview_df['Vertical_min'] == 1.0].index\n",
    "\n",
    "    for i in vmin_pc:\n",
    "        vmin_dict[i] = {}\n",
    "\n",
    "        for v in vars_g:\n",
    "            vmin_dict[i][v] = False\n",
    "    \n",
    "    return vmin_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check whether data should be aligned if vmin = 1, and align if so if has not been done already\n",
    "def check_alignment(data_dict, pc, var, align_and_nan, vmin_dict):\n",
    "    \n",
    "    xarr = data_dict[pc]['data']\n",
    "    xarr_var = xarr[var].data\n",
    "    \n",
    "    vmin = float(xarr.attrs['geospatial_vertical_min'])\n",
    "\n",
    "    if vmin == 0:\n",
    "        print(f'Platform: {pc}; Vertical min: {vmin}; Var: {var}')\n",
    "        \n",
    "    elif vmin==1 and vmin_dict[pc][var]==False and align_and_nan: \n",
    "        # shift to the right and add nan in first position \n",
    "        print(f'Platform: {pc}; Vertical min: {vmin}; Var: {var} --> aligning and add nan')        \n",
    "        data_dict[pc]['data'][var].data = adjust_with_vmin(xarr_var, value=np.nan)\n",
    "        vmin_dict[pc][var] = True # to avoid doing hte vmin adjustment for this pc/var more than once  \n",
    "        \n",
    "    elif vmin==1 and vmin_dict[pc][var]==False and not align_and_nan: \n",
    "        # No need to shift, this occurred already in the data extraction\n",
    "        print(f'Platform: {pc}; Vertical min: {vmin}; Var: {var} --> data has been aligned already')\n",
    "        vmin_dict[pc][var] = True # to avoid doing hte vmin adjustment for this pc/var more than once\n",
    "    \n",
    "    \n",
    "#     return data_dict[pc], vmin_dict[pc][var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterbyDepthAndIndices(data_dict_yr, metadata_yr, vmin_dict_yr, df_filtered):\n",
    "    \n",
    "    print(f'\\n3) Selected DEPTH: {depth_g}m')\n",
    "    \n",
    "    for year in data_dict_yr.keys():\n",
    "        print(year)\n",
    "        filtered_xarr_dict[year] = {}\n",
    "        \n",
    "        # Select available platforms ONLY of the specific year\n",
    "        df_year = df_filtered[pd.DatetimeIndex(df_filtered['Time']).year == 2003] # select only those rows of that year\n",
    "        pc_year = df_year['Platform'].unique()\n",
    "\n",
    "        for pc in pc_year:\n",
    "            print(pc)\n",
    "            # Generate a filtered xarray with all variables for selected Platform, for a certain DEPTH range\n",
    "            \n",
    "            if metadata_yr[year][pc]['depth_m_v1']==0: align_and_nan = True\n",
    "            else: align_and_nan = False\n",
    "\n",
    "            for v in vars_g: \n",
    "                check_alignment(data_dict_yr[year], pc, v, align_and_nan, vmin_dict_yr[year]) \n",
    "\n",
    "            filtered_xarr_dict[year][pc] = filter_xarr_DEPTH(df_filtered, \n",
    "                                                             data_dict_yr[year],\n",
    "                                                             platform=pc,\n",
    "                                                             depth_range=[depth_g, depth_g])\n",
    "            display(filtered_xarr_dict[year][pc])\n",
    "        print('\\n')\n",
    "    \n",
    "    return filtered_xarr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter XARRAY based on platform, Var and DEPTH\n",
    "def filter_xarr_DEPTH(df_toPlot, data_dict, platform, depth_range):\n",
    "   \n",
    "    # find indices for each platform for the selected data\n",
    "    index = df_toPlot[df_toPlot['Platform']==platform].index.tolist()\n",
    "        \n",
    "    # Filer data using the indexes of the filtered elements\n",
    "    xarr_sel = data_dict[platform]['data'].isel(TIME=index,\n",
    "                                                LATITUDE=index,\n",
    "                                                LONGITUDE=index,\n",
    "                                                DEPTH=slice(depth_range[0], depth_range[1]+1))\n",
    "    return xarr_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 25\n",
    "bbox = \"-20, 40, 50, 85\"\n",
    "time1 = '20040101' \n",
    "time2 = '20040131' #'20040131' \n",
    "vars_sel = 'TEMP'\n",
    "formats = 'csv'\n",
    "group = False\n",
    "\n",
    "checkParams(depth, bbox, time1, time2, vars_sel, group, formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of Norwegian Marine Data Centre (NMDC) data server\n",
    "nmdc_url = 'http://opendap1.nodc.no/opendap/physics/point/yearly/' \n",
    "\n",
    "# URL of Terradue Cloud Platform Hyrax server\n",
    "# Ellip user account and VPN setup required\n",
    "t2_hyrax_url = 'https://opendap.terradue.com/hyrax/data/subset_2003/'\n",
    "\n",
    "urls = {}\n",
    "urls['nmdc'] = [nmdc_url, '']\n",
    "urls['t2_hyrax'] = [t2_hyrax_url, '.nc4']\n",
    "\n",
    "#========================================================\n",
    "# Define below the URL to use (either 'nmdc' or 't2_hyrax'):\n",
    "url_info = urls['nmdc']\n",
    "#========================================================\n",
    "\n",
    "print('Server URL and URL suffix:', url_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of NetCDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePositionDF(position_dict):\n",
    "    # Load locations (LONG & LAT) and TIME of all measurements in a position_df_raw (includes duplicates)\n",
    "\n",
    "    position_df_raw = pd.DataFrame() \n",
    "\n",
    "    for key in position_dict.keys():\n",
    "        test = pd.DataFrame()\n",
    "    \n",
    "        test['Longitude_WGS84'] = position_dict[key]['data']['LONGITUDE'].data.astype(float)\n",
    "        test['Latitude_WGS84'] = position_dict[key]['data']['LATITUDE'].data.astype(float)\n",
    "        test['Time'] = position_dict[key]['data']['TIME'].data.astype(float)\n",
    "        test['Platform'] = key\n",
    "\n",
    "        # Convert TIME from float to datetime\n",
    "        test['Time'] = [start_date + timedelta(t) for t in test.loc[:,'Time']]\n",
    "        length = len(test[test['Platform']==key])\n",
    "        print(f'Platform {key}: {length} measurement locations.')\n",
    "        \n",
    "        position_df_raw = position_df_raw.append(test) \n",
    "        \n",
    "    position_df_raw['Index_ABS'] = np.arange(0,len(position_df_raw))\n",
    "    position_df_raw = position_df_raw.rename_axis(\"Index_Relative\")\n",
    "    \n",
    "    # Now remove duplicates\n",
    "    duplicates = position_df_raw[position_df_raw.duplicated(subset='Time') == True]\n",
    "    \n",
    "    position_df_temp = position_df_raw.drop_duplicates(subset=['Time'])\n",
    "    \n",
    "    print(f'Merged dataframe with all platforms. Total of {len(position_df_raw)} measurement positions')\n",
    "    print(f'Duplicates: \\t{len(duplicates)} / {len(position_df_raw)} \\nRemaining: \\t{len(position_df_temp)} / {len(position_df_raw)}')\n",
    "\n",
    "    return position_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dds_year_dict = {}\n",
    "pos_year_dict = {}\n",
    "global position_df\n",
    "position_df = pd.DataFrame()\n",
    "\n",
    "for year in range(year1, year2+1): # need to do a for loop over the years as the data is saved in years on the server\n",
    "    print('\\nWorking on year:', year)\n",
    "    \n",
    "    # Retrieval of DDS info\n",
    "    dds_year_dict[year] = getDDS(url_info, year) # dds_year_dict[year] replaced pc_dim_dict \n",
    "    pprint.pprint(dds_year_dict[year])\n",
    "    \n",
    "    # Extract all platform_codes for that year\n",
    "    platform_codes = [pc for pc in dds_year_dict[year].keys()]\n",
    "    print(f'Available platforms in given year {year}: {platform_codes}')\n",
    "\n",
    "    # Create position_dict\n",
    "    pos_year_dict[year] = getPositionDict(dds_year_dict[year], url_info, year) # pos_year_dict[year] replaced position_dict\n",
    "#     pprint.pprint(pos_year_dict[year])\n",
    "\n",
    "    # Match and merge LAT, LONG and TIME of positions in a position_df dataframe\n",
    "    print('Now makePositionDF')\n",
    "    position_df_temp = makePositionDF(pos_year_dict[year])\n",
    "    \n",
    "    position_df = position_df.append(position_df_temp)\n",
    "    display(position_df)\n",
    "\n",
    "print('COMBINED position_df')\n",
    "display(position_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by BBOX and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by BBOX and Time\n",
    "df_filtered = filterBBOXandTIME(position_df, time1, time2)\n",
    "display(df_filtered)\n",
    "\n",
    " # Dictionary of indices\n",
    "index_dict = getIndices(df_filtered)\n",
    "pprint.pprint(index_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing: Load and Plot selected Data (Variables within DEPTH range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_dict_yr, metadata_yr, vmin_dict_yr, filtered_xarr_dict\n",
    "data_dict_yr = {}\n",
    "metadata_yr = {}\n",
    "vmin_dict_yr = {}\n",
    "\n",
    "for year in range(year1, year2+1): # need to do a for loop over the years as the data is saved in years on the server\n",
    "    \n",
    "    # Extract all platform_codes for that year\n",
    "    pc_sel = [pc for pc in dds_year_dict[year].keys()]\n",
    "    print(f'Working on year: {year} - Available platforms: {pc_sel}')\n",
    "\n",
    "    data_dict_yr[year], metadata_yr[year] = extractVARsAndDepth(pc_sel, pos_year_dict[year], dds_year_dict[year], url_info, year) \n",
    "\n",
    "    print(f'Attributes Year: {year}')\n",
    "    # Create overview dataframe\n",
    "    overview_df = pd.DataFrame()\n",
    "    overview_df = getAttributes(overview_df, data_dict_yr[year])\n",
    "    print('overview_df')\n",
    "    display(overview_df)\n",
    "\n",
    "    # Generate vmin dictionary (needed to avoid doing the vmin adjustment more than once)\n",
    "    vmin_dict_yr[year] = getVminDict(overview_df)\n",
    "    \n",
    "print('\\n2) ===================Printing Results:')\n",
    "print('\\n2.1) data_dict_yr')\n",
    "pprint.pprint(data_dict_yr)\n",
    "print('\\n2.2) metadata_yr')\n",
    "pprint.pprint(metadata_yr)\n",
    "print('\\n2.3) vmin_dict_yr')\n",
    "pprint.pprint(vmin_dict_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_yr[2003]['GS']['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Depth and Indices (generated by BBOX and Time indices)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by Depth and Indices (generated by BBOX and Time indices) \n",
    "filtered_xarr_dict = {}\n",
    "filtered_xarr_dict = filterbyDepthAndIndices(data_dict_yr, metadata_yr, vmin_dict_yr, df_filtered)\n",
    "print('\\n4) filtered_xarr_dict', filtered_xarr_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation of available platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregatePlatforms(filtered_xarr_dict, year):\n",
    "    # Dictionary of variables for each platform\n",
    "    depth_arr = []\n",
    "\n",
    "    for year in [2003]:\n",
    "        print(year)\n",
    "        data_var_dict_yr[year] = {}\n",
    "\n",
    "        for pc in filtered_xarr_dict[year].keys():\n",
    "\n",
    "            data_var_dict_yr[year][pc] = {}\n",
    "            data = filtered_xarr_dict[year][pc]\n",
    "\n",
    "            depth_dim_pc = data.dims[\"DEPTH\"]\n",
    "            depth_arr.append(depth_dim_pc)\n",
    "\n",
    "            print(f'PC {pc}\\tFiltered Dims: TIME={data.dims[\"TIME\"]}, DEPTH={data.dims[\"DEPTH\"]}')\n",
    "\n",
    "            for var in vars_g:\n",
    "                data_var_dict_yr[year][pc][var] = filtered_xarr_dict[year][pc][var]\n",
    "\n",
    "        assert all(x==depth_arr[0] for x in depth_arr), 'ERROR, the DEPTH dimensions must be equal.'\n",
    "        #display(data_var_dict_yr[year])\n",
    "        \n",
    "    return data_var_dict_yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_var_dict_yr = {}\n",
    "data_var_dict_yr = aggregatePlatforms(filtered_xarr_dict, year)\n",
    "display(data_var_dict_yr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Arrays across platforms and years \n",
    "Now aggregate and plot each variable: on the y-axis is shown the TIME of the measurement (in float format, which needs to be converted to datetime format), and on the x-axis is the DEPTH of the measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_var_dict_yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeArrays(data_var_dict_yr):\n",
    "    # Combine arrays across platforms, for each variable\n",
    "        \n",
    "    for var in vars_g:\n",
    "\n",
    "        merged_arr[var] = xr.merge([data_var_dict_yr[y][pc][var] for y in data_var_dict_yr.keys() for pc in data_var_dict_yr[y].keys()] )  \n",
    "        \n",
    "    return merged_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_arr = {}\n",
    "merged_arr = mergeArrays(data_var_dict_yr)\n",
    "merged_arr['TEMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter by BBOX, adding Time Range filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define here start and end date in the format [day,month,year]\n",
    "time_start = [1,12,2003] \n",
    "time_end = [31,1,2004] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = datetime(time_start[2], time_start[1], time_start[0])\n",
    "time_end = datetime(time_end[2], time_end[1], time_end[0])\n",
    "\n",
    "position_df_bbox_timerange = position_df_bbox.loc[(position_df_bbox['Time']>=time_start) & \n",
    "                                                  (position_df_bbox['Time']<=time_end)]\n",
    "\n",
    "# Print filtering results on original dataframe\n",
    "print('Year:', year)\n",
    "print('BBOX:', bbox_key)\n",
    "time_filter_str = f'{time_start.strftime(\"%Y%m%d\")}-{time_end.strftime(\"%Y%m%d\")}'\n",
    "print(f'Time Filter: {time_filter_str}')\n",
    "sel_outof_all = f'{len(position_df_bbox_timerange)} out of {len(position_df)}.'\n",
    "print(f'Selected positions (out of available positions): {sel_outof_all}')\n",
    "\n",
    "display(position_df_bbox_timerange)\n",
    "\n",
    "title = f'Filtered data: {bbox_key} and Time={time_filter_str}'\n",
    "plotInteractive(position_df_bbox_timerange, title, 'Longitude', 'Latitude', xlim, ylim, bbox_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print or Export Filtered Positions\n",
    "Uncomment the rows below if you want to display or export to CSV the filtered dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print filtered dataframe\n",
    "# pd.set_option('display.max_rows', None) # pd.set_option('display.max_rows', 10) # to restore first- and last- 5 rows to display\n",
    "# display(position_df_bbox_timerange)\n",
    "\n",
    "# # Save dataframe to csv\n",
    "# data_output = os.path.join(os.getcwd(), 'data_output')\n",
    "# if not os.path.exists(data_output): os.mkdir(data_output)\n",
    "# csvname = os.path.join(data_output, f'filtered_{pc}_df.csv')\n",
    "# position_df_bbox_timerange.to_csv(csvname, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zoom in on Filtered Positions\n",
    "Plot filtered positions with the relavite extent of those positions only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim_small = (min(position_df_bbox_timerange['Longitude'])-1, max(position_df_bbox_timerange['Longitude'])+1)\n",
    "ylim_small = (min(position_df_bbox_timerange['Latitude'])-1, max(position_df_bbox_timerange['Latitude'])+1)\n",
    "title = f'Filtered data: {bbox_key} and Time={time_filter_str}'\n",
    "plotInteractive(position_df_bbox_timerange, title, 'Longitude', 'Latitude', xlim_small, ylim_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Filtered Dataframe (*df_toPlot*) to be used for online querying of the filtered positions\n",
    "Define the filtered dataframe (eg *position_df_bbox*, *position_df_bbox_timerange*), to be named **df_toPlot**, and the dictionary of indices of filtered data (to be named **index_dict**), to use for further filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataframe to plot based on one of the previously defined filters\n",
    "df_toPlot = position_df_bbox_timerange # or position_df / position_df_bbox\n",
    "\n",
    "sel_outof_all = f'{len(df_toPlot)} / {len(position_df)}.'\n",
    "\n",
    "print(f'- Filters: \"BBOX={bbox_key}\" and \"Time={time_filter_str}\"\\n- Filtered / All (out of available positions): {sel_outof_all}')\n",
    "\n",
    "display(df_toPlot)\n",
    "\n",
    "# Dictionary of indices\n",
    "index_dict = {}\n",
    "\n",
    "for pc in df_toPlot['Platform'].unique():\n",
    "    index_dict[pc] = df_toPlot[df_toPlot['Platform']==pc].index.tolist()\n",
    "\n",
    "index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_toPlot[df_toPlot['Platform']=='JH'])\n",
    "df_toPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Up to here is comparable to the execution \"extractor-tool --depth 25 --bbox \"-20, 40, 50, 85\" --time1 20031201 --time2 20040131 --vars 'TEMP' --format 'csv'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "The CTD data is extracted based on the following parameters: Variables, Depth range, and Platform code. \n",
    "\n",
    "Afterwards, an additional filtering is applied based on the list of indices that will be extracted from the dictionary of positions (*df_toPlot*). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Variable(s)\n",
    "vars_main = ['PRES', 'TEMP', 'PSAL', 'CNDC'] \n",
    "\n",
    "# Define the selection of variable to use for the analysis\n",
    "vars_sel = ['TEMP']#, 'PRES'] #, 'CNDC', 'PSAL']; \n",
    "assert all([elem in vars_main for elem in vars_sel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define depths limits\n",
    "depth1 = 25\n",
    "depth2 = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Platform(s) - Select either one of the two options below  \n",
    "\n",
    "#================================================\n",
    "# Option A) Use data from ALL available platforms\n",
    "pc_sel = df_toPlot['Platform'].unique()\n",
    "\n",
    "# Option B) Use data from only ONE platform\n",
    "# pc_sel = ['AA']; assert pc_sel in df_toPlot['Platform'].unique(), 'ERROR: platform not available in given year.'\n",
    "#================================================\n",
    "\n",
    "# Create string for output name\n",
    "if len(pc_sel) == 1: pc_str = pc_sel[0]\n",
    "else: pc_str = \"-\".join(pc_sel)\n",
    "    \n",
    "# Print selection\n",
    "print(f'Platform(s) selected: {pc_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create **data_dict** dictionary \n",
    "Data and their attributes are read iteratively for selected platform(s), and saved into a dictionary *data_dict* which contains:\n",
    "* the actual data, loaded into an **xarray** for data handling, analysis and visualisation\n",
    "* the campaign's main attributes: platform code & name, data type, title, instrument, longitude & latitude, and vertical min & max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2003, 2004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dds_year_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    \n",
    "    # Extract all platform_codes for that year\n",
    "    pc_sel = [pc for pc in dds_year_dict[year].keys()]\n",
    "    print(f'Working on year: {year} - Available platforms: {pc_sel}')\n",
    "        \n",
    "    \n",
    "    \n",
    "    data_dict = {}\n",
    "    metadata = {}\n",
    "\n",
    "    for pc in pc_sel:\n",
    "\n",
    "        metadata[pc] = {}\n",
    "\n",
    "        v_min = int(float(position_dict[pc]['data_attr'][6]))\n",
    "        metadata[pc]['vmin'] = v_min\n",
    "        metadata[pc]['depth_abs_v1'] = 0 # this is fixed\n",
    "        metadata[pc]['depth_abs_v2'] = pc_dim_dict[pc]['DEPTH'] # this is fixed\n",
    "\n",
    "        # ==============================================================================\n",
    "        \"\"\"\n",
    "        Define here the DEPTH range of your selection, in meters. Note that either:\n",
    "        - 'depth_m_v1' is equal to the lower bound (ie index=0), or \n",
    "        - 'depth_m_v2' is equal to the upper bound (ie index=-1)\n",
    "        \"\"\"\n",
    "        metadata[pc]['depth_m_v1'] = 0 # depth1\n",
    "        metadata[pc]['depth_m_v2'] = depth2 # pc_dim_dict[pc]['DEPTH']\n",
    "        # ==============================================================================\n",
    "\n",
    "        # assert metadata[pc]['depth_m_v1'] < metadata[pc]['depth_m_v2'], 'ERROR: the lower bound must be lower than the higher bound' \n",
    "        # assert metadata[pc]['depth_m_v1'] == 0 or metadata[pc]['depth_m_v2'] == pc_dim_dict[pc]['DEPTH'], 'ERROR: one of the two values must be equal to one of the lower/upper bounds'\n",
    "\n",
    "        #     print(f'DEPTH range of interest (meters): {metadata[pc][\"depth_m_v1\"]} - {metadata[pc][\"depth_m_v2\"]}')\n",
    "\n",
    "        # the start and stop values are adjusted based on the vmin value\n",
    "        if metadata[pc]['vmin'] == 1: \n",
    "            if metadata[pc]['depth_m_v1'] == 0: # \n",
    "                metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1'] # the same\n",
    "                metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2'] # the same, so I have the right size. When I shift and add the nan, I get rid of further element on the right\n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2']# - 1\n",
    "\n",
    "            elif metadata[pc]['depth_m_v1'] != 0: \n",
    "                metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1'] - 1 # start one element before\n",
    "                metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2'] - 1 # last element is excluded, ie stop one element before. But then I'll have to remoove one element\n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2'] - metadata[pc]['depth_m_v1'] - 1 \n",
    "\n",
    "        else:\n",
    "            metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1']\n",
    "            metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2']\n",
    "\n",
    "            if metadata[pc]['depth_m_v1'] == 0: # \n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2']\n",
    "\n",
    "            elif metadata[pc]['depth_m_v1'] != 0: \n",
    "                metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2'] - metadata[pc]['depth_m_v1']\n",
    "\n",
    "        metadata[pc]['depth_newindex4xr_v1'] = 0\n",
    "\n",
    "        pprint.pprint(metadata[pc])\n",
    "        print(f'{pc} DEPTH range of interest (adjusted with vmin): {metadata[pc][\"depth_newindex_v1\"]} - {metadata[pc][\"depth_newindex_v2\"]}')\n",
    "\n",
    "        fix_lab = f'58{pc}_CTD_{year}' # platform_codes and year are defined at the beginning of the notebook \n",
    "\n",
    "        # Get coordinates (needed for keeping hte correct structure, and for plotting) \n",
    "        coords_str = getQueryString(pc_dim_dict[pc], keylist = ['TIME', 'LATITUDE', 'LONGITUDE']) # list the coordinates you want\n",
    "\n",
    "        # Extract TIME and DEPTH dimension for queries \n",
    "        time_dims = getQuery(pc, start=0, stop=pc_dim_dict[pc]['TIME'])\n",
    "        depth_dims = getQuery(pc, start=metadata[pc]['depth_newindex_v1'], stop=metadata[pc]['depth_newindex_v2'])#; print(depth_dims)\n",
    "\n",
    "        # join TIME and DEPTH for Variables\n",
    "        var_str_ALL = []\n",
    "        for v in vars_sel: var_str_ALL = np.append(var_str_ALL, f'{v}{time_dims}{depth_dims}')\n",
    "        queries_vars = ','.join(var_str_ALL)\n",
    "\n",
    "        # Build url and url with queries (url_q)\n",
    "        url = f'{url_info[0]}{fix_lab}.nc{url_info[1]}?{coords_str}' \n",
    "        url_q = f'{url},{queries_vars}'; print(f'Platform {pc} URL:', url_q)\n",
    "\n",
    "        remote_data, data_attr = fetch_data(url_q, year)\n",
    "\n",
    "        data_dict[pc] = {'data': remote_data, \n",
    "                         'data_attr': data_attr}\n",
    "\n",
    "        print(f'{data_attr}\\n')\n",
    "\n",
    "    # display(data_dict)\n",
    "    print(f'Checking the existing campaigns in the dictionary: {list(data_dict.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Overview Dataframe with Platforms' Attributes\n",
    "An overview dataframe *overview_df* is then generated to show the detailed information about each campaign at sea: platform code & name, data type, title, instrument, longitude & latitude, and vertical min & max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database with the selected campaigns & years\n",
    "overview_df = pd.DataFrame()\n",
    "overview_df = getAttributes(overview_df, data_dict)\n",
    "overview_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract / define the variables to use for the analysis\n",
    "print('\\nPrinting DEPTH range for analyis:')\n",
    "assert len(np.unique([metadata[k][\"depth_m_v1\"] for k in metadata.keys()])==1)\n",
    "assert len(np.unique([metadata[k][\"depth_m_v2\"] for k in metadata.keys()])==1)\n",
    "\n",
    "for k in data_dict.keys():\n",
    "    print(f'{k}; DEPTH filtered: {metadata[k][\"depth_m_v1\"]}-{metadata[k][\"depth_m_v2\"]}m; VARS: {list(data_dict[k][\"data\"].variables)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate vmin dictionary (needed to avoid doing the vmin adjustment more than once)\n",
    "vmin_dict = {}\n",
    "\n",
    "# select only those platforms where vmin == 1\n",
    "vmin_pc = overview_df[overview_df['Vertical_min'] == 1.0].index\n",
    "\n",
    "for i in vmin_pc:\n",
    "    vmin_dict[i] = {}\n",
    "    \n",
    "    for v in vars_sel:\n",
    "        vmin_dict[i][v] = False\n",
    "\n",
    "vmin_dict   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by DEPTH and selected Indices\n",
    "Apply the previously defined selection of indices (filtered based on BBOX and Time Range), and add filtering by DEPTH range. \n",
    "\n",
    "The output of this operation is a *filtered_xarr_dict* dictionary containing xarray datasets for each platform, containing the variable at the specified DEPTH range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data filtered by DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_xarr_dict = {}\n",
    "\n",
    "print(f'Selected range of DEPTH: {depth1} - {depth2}m\\n')\n",
    "for pc in data_dict.keys():\n",
    "\n",
    "    # Generate a filtered xarray with all variables for selected Platform, for a certain DEPTH range\n",
    "    if metadata[pc]['depth_m_v1']==0: align_and_nan = True\n",
    "    else: align_and_nan = False\n",
    "\n",
    "    for v in vars_sel: \n",
    "        check_alignment(data_dict, pc, v, align_and_nan, vmin_dict)\n",
    "\n",
    "    filtered_xarr_dict[pc] = filter_xarr_DEPTH(df_toPlot, \n",
    "                                               data_dict,\n",
    "                                               platform=pc,\n",
    "                                               depth_range=[depth1, depth2])\n",
    "    # display(filtered_xarr_dict[pc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation of Available Platforms\n",
    "Two xarray datasets can be merged if they have the same structure, i.e. dimensions. First check the dimensions of DEPTH are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of variables for each platform\n",
    "data_var_dict = {}\n",
    "depth_arr = []\n",
    "\n",
    "for pc in data_dict.keys():\n",
    "    \n",
    "    data_var_dict[pc] = {}\n",
    "    data = filtered_xarr_dict[pc]\n",
    "    \n",
    "    depth_dim_pc = data.dims[\"DEPTH\"]\n",
    "    depth_arr.append(depth_dim_pc)\n",
    "    \n",
    "    print(f'PC {pc}\\tFiltered Dims: TIME={data.dims[\"TIME\"]}, DEPTH={data.dims[\"DEPTH\"]}')\n",
    "    \n",
    "    for var in vars_sel:\n",
    "        data_var_dict[pc][var] = filtered_xarr_dict[pc][var]\n",
    "        \n",
    "assert all(x==depth_arr[0] for x in depth_arr), 'ERROR, the DEPTH dimensions must be equal.'\n",
    "# display(data_var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now aggregate and plot each variable: on the y-axis is shown the TIME of the measurement (in float format, which needs to be converted to datetime format), and on the x-axis is the DEPTH of the measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine arrays across platforms, for each variable\n",
    "merged_arr = {}\n",
    "\n",
    "for var in vars_sel:\n",
    "    \n",
    "    merged_arr[var] = xr.merge([data_var_dict[pc][var] for pc in data_dict.keys()])  \n",
    "        \n",
    "    title = f'Var={var} (Merged Platforms)\\nFilter: Time Range={time_filter_str};\\nBBOX={bbox_key}; Depth Range {depth1}-{depth2}m;\\nSel/All={sel_outof_all}'\n",
    "\n",
    "    plotVar_MergedPlatforms(merged_arr[var], var, title=title)\n",
    "    # display(merged_arr[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine arrays across platforms, with ALL variables\n",
    "merged_arr_vars = xr.merge([data_var_dict[pc][var] for pc in data_dict.keys() for var in vars_sel]) \n",
    "merged_arr_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = merged_arr_vars.to_dataframe().reset_index()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m['DEPTH'] = depth1\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[['TIME','DEPTH','TEMP']].to_csv('/workspace/INTAROS/iaos-CTD-extract-from-opendap/exported_data/test_temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['AA']['data'].isel(TIME=np.arange(0,2),\n",
    "                             LATITUDE=np.arange(0,2),\n",
    "                             LONGITUDE=np.arange(0,2),\n",
    "                             DEPTH=np.arange(0,1)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['AA']['data'].isel(TIME=np.arange(0,2),\n",
    "                             LATITUDE=np.arange(0,2),\n",
    "                             LONGITUDE=np.arange(0,2),\n",
    "                             DEPTH=np.arange(0,1)\n",
    "                            ).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_var_dict[pc]['TEMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Recap:\n",
    "* ```data_dict[pc]['data']```: contains the xarray extracted from the url (eg from **0** to **depth2**, for ALL locations)\n",
    "* ```filtered_xarr```: contains the xarray filtered from ```data_dict[pc]['data']``` (ie from **depth1** to **depth2**, for filtered locations (BBOX and time_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output dir\n",
    "data_output = os.path.join(os.getcwd(), 'data_output')\n",
    "if not os.path.exists(data_output): os.mkdir(data_output)\n",
    "\n",
    "# File name (without file extension)\n",
    "fname = os.path.join(data_output, \n",
    "                     f'Scientist_pc={pc_str}_BBOX={bbox_key}_MMYYYY={time_filter_str}_d={depth1}-{depth2}m_var={\"_\".join(vars_sel)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to NetCDF\n",
    "netcdfname = fname + '.nc.nc4'\n",
    "merged_arr['TEMP'].to_netcdf(path=netcdfname,\n",
    "                             mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to CSV\n",
    "##### Create Dataframe of Filtered XARRAY\n",
    "This step is implemented to generate a CSV-structured dataframe, to then export to a CSV file, which is the input expected by the RGeostats module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with filtered data and columns ['LATITUDE', 'LONGITUDE', 'TIME', 'TEMP', 'DEPTH']\n",
    "filtered2csv_multiDepths = pd.DataFrame() \n",
    "\n",
    "for pc in data_dict.keys():\n",
    "\n",
    "    for d in range(depth1, depth2+1):\n",
    "\n",
    "        # Create temporary dataframe\n",
    "        temp = pd.DataFrame()\n",
    "\n",
    "        data_depth_sel = data_dict[pc]['data'].isel(TIME=index_dict[pc],\n",
    "                                                    LATITUDE=index_dict[pc],\n",
    "                                                    LONGITUDE=index_dict[pc],\n",
    "                                                    DEPTH=slice(d, d+1))\n",
    "\n",
    "        for col in ['LONGITUDE', 'LATITUDE', 'TIME']:\n",
    "            temp[col.title()] = data_depth_sel[col].data.astype(float) \n",
    "\n",
    "        if 'TEMP' in vars_sel: temp['Temperature'] = data_depth_sel['TEMP'].data.astype(float) \n",
    "        if 'CNDC' in vars_sel: temp['Conductivity'] = data_depth_sel['CNDC'].data.astype(float) \n",
    "        if 'PSAL' in vars_sel: temp['Salinity'] = data_depth_sel['PSAL'].data.astype(float) \n",
    "\n",
    "        temp['Depth'] = d \n",
    "        temp['Vessel_name'] = pc \n",
    "\n",
    "        filtered2csv_multiDepths = filtered2csv_multiDepths.append(temp, ignore_index=True)\n",
    "    \n",
    "# Rename index column with 'rank'\n",
    "filtered2csv_multiDepths = filtered2csv_multiDepths.rename_axis(\"rank\")\n",
    "\n",
    "display(filtered2csv_multiDepths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assign *Profil_id* to the unique positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find pair of unique coordinates \n",
    "unique_pos = filtered2csv_multiDepths.groupby(['Longitude','Latitude']).size().reset_index().rename(columns={0:'count'})\n",
    "\n",
    "prof_id = 1\n",
    "for long, lat in zip(unique_pos['Longitude'], unique_pos['Latitude']):\n",
    "    \n",
    "    # Define condition\n",
    "    cond = (filtered2csv_multiDepths['Longitude'] == long) & (filtered2csv_multiDepths['Latitude'] == lat)\n",
    "#     display(filtered2csv_multiDepths.loc[cond])\n",
    "    \n",
    "    # Assign unique Profil_id \n",
    "    filtered2csv_multiDepths.loc[cond,'Profil_id'] = prof_id\n",
    "    prof_id += 1\n",
    "\n",
    "# Convert to integer\n",
    "filtered2csv_multiDepths = filtered2csv_multiDepths.astype({'Profil_id': int})\n",
    "display(filtered2csv_multiDepths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe to CSV\n",
    "csvname = fname + '.csv'\n",
    "filtered2csv_multiDepths.to_csv(csvname, sep=',', header=True)\n",
    "print('Output filename:', csvname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_intaros]",
   "language": "python",
   "name": "conda-env-env_intaros-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
